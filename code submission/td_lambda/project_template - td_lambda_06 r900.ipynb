{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swifter\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ce7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import pickle\n",
    "import random\n",
    "import sqlite3\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def actRules(self, state):\n",
    "        return 1\n",
    "    \n",
    "    def actNaively(self):\n",
    "        return 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY PARAMETERS\n",
    "\n",
    "lambda_value = 0.6\n",
    "table_name = f\"gs_results_td_lambda_{lambda_value}\".replace(\".\", \"\")\n",
    "directory = \"e-greedy/agents\" #dir to create\n",
    "gs_db_name = 'e-greedy/grid_search_greedy_r900.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a9f54",
   "metadata": {},
   "source": [
    "### RADIUS SET TO 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341de44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Car:\n",
    "    def __init__(self, tyre=\"Intermediate\"):\n",
    "        self.default_tyre = tyre\n",
    "        self.possible_tyres = [\"Ultrasoft\", \"Soft\", \"Intermediate\", \"Fullwet\"]\n",
    "        self.pitstop_time = 23\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.change_tyre(self.default_tyre)\n",
    "    \n",
    "    \n",
    "    def degrade(self, w, r):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            self.condition *= (1 - 0.0050*w - (2500-r)/90000)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            self.condition *= (1 - 0.0051*w - (2500-r)/93000)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            self.condition *= (1 - 0.0052*abs(0.5-w) - (2500-r)/95000)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            self.condition *= (1 - 0.0053*(1-w) - (2500-r)/97000)\n",
    "        \n",
    "        \n",
    "    def change_tyre(self, new_tyre):\n",
    "        assert new_tyre in self.possible_tyres\n",
    "        self.tyre = new_tyre\n",
    "        self.condition = 1.00\n",
    "    \n",
    "    \n",
    "    def get_velocity(self):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            vel = 80.7*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            vel = 80.1*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            vel = 79.5*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            vel = 79.0*(0.2 + 0.8*self.condition**1.5)\n",
    "        return vel\n",
    "\n",
    "    \n",
    "class Track:\n",
    "    def __init__(self, car=Car()):\n",
    "        # self.radius and self.cur_weather are defined in self.reset()\n",
    "        self.total_laps = 162\n",
    "        self.car = car\n",
    "        self.possible_weather = [\"Dry\", \"20% Wet\", \"40% Wet\", \"60% Wet\", \"80% Wet\", \"100% Wet\"]\n",
    "        self.wetness = {\n",
    "            \"Dry\": 0.00, \"20% Wet\": 0.20, \"40% Wet\": 0.40, \"60% Wet\": 0.60, \"80% Wet\": 0.80, \"100% Wet\": 1.00\n",
    "        }\n",
    "        self.p_transition = {\n",
    "            \"Dry\": {\n",
    "                \"Dry\": 0.987, \"20% Wet\": 0.013, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"20% Wet\": {\n",
    "                \"Dry\": 0.012, \"20% Wet\": 0.975, \"40% Wet\": 0.013, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"40% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.012, \"40% Wet\": 0.975, \"60% Wet\": 0.013, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"60% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.012, \"60% Wet\": 0.975, \"80% Wet\": 0.013, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"80% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.012, \"80% Wet\": 0.975, \"100% Wet\": 0.013\n",
    "            },\n",
    "            \"100% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.012, \"100% Wet\": 0.988\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        # self.radius = np.random.randint(600,1201)\n",
    "        self.radius = 900\n",
    "        self.cur_weather = np.random.choice(self.possible_weather)\n",
    "        self.is_done = False\n",
    "        self.pitstop = False\n",
    "        self.laps_cleared = 0\n",
    "        self.car.reset()\n",
    "        return self._get_state()\n",
    "    \n",
    "    \n",
    "    def _get_state(self):\n",
    "        return [self.car.tyre, self.car.condition, self.cur_weather, self.radius, self.laps_cleared]\n",
    "        \n",
    "    \n",
    "    def transition(self, action=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (int):\n",
    "                0. Make a pitstop and fit new ‘Ultrasoft’ tyres\n",
    "                1. Make a pitstop and fit new ‘Soft’ tyres\n",
    "                2. Make a pitstop and fit new ‘Intermediate’ tyres\n",
    "                3. Make a pitstop and fit new ‘Fullwet’ tyres\n",
    "                4. Continue the next lap without changing tyres\n",
    "        \"\"\"\n",
    "        ## Pitstop time will be added on the first eight of the subsequent lap\n",
    "        time_taken = 0\n",
    "        if self.laps_cleared == int(self.laps_cleared):\n",
    "            if self.pitstop:\n",
    "                self.car.change_tyre(self.committed_tyre)\n",
    "                time_taken += self.car.pitstop_time\n",
    "                self.pitstop = False\n",
    "        \n",
    "        ## The environment is coded such that only an action taken at the start of the three-quarters mark of each lap matters\n",
    "        if self.laps_cleared - int(self.laps_cleared) == 0.75:\n",
    "            if action < 4:\n",
    "                self.pitstop = True\n",
    "                self.committed_tyre = self.car.possible_tyres[action]\n",
    "            else:\n",
    "                self.pitstop = False\n",
    "        \n",
    "        self.cur_weather = np.random.choice(\n",
    "            self.possible_weather, p=list(self.p_transition[self.cur_weather].values())\n",
    "        )\n",
    "        # we assume that degration happens only after a car has travelled the one-eighth lap\n",
    "        velocity = self.car.get_velocity()\n",
    "        time_taken += (2*np.pi*self.radius/8) / velocity\n",
    "        reward = 0 - time_taken\n",
    "        self.car.degrade(\n",
    "            w=self.wetness[self.cur_weather], r=self.radius\n",
    "        )\n",
    "        self.laps_cleared += 0.125\n",
    "        \n",
    "        if self.laps_cleared == self.total_laps:\n",
    "            self.is_done = True\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        return reward, next_state, self.is_done, velocity\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.transition(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_car = Car()\n",
    "env = Track(new_car)\n",
    "\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6d6a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sanity check..\n",
    "\n",
    "state = env.reset()    \n",
    "done = False\n",
    "G = 0\n",
    "while not done:\n",
    "    action = agent.actNaively()\n",
    "    reward, next_state, done, velocity = env.transition(action)\n",
    "    # added velocity for sanity check\n",
    "    # state = deepcopy(next_state)\n",
    "    state = next_state\n",
    "    G += reward\n",
    "\n",
    "print(\"G: %d\" % G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81524f23",
   "metadata": {},
   "source": [
    "## TDLambda Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebdc93",
   "metadata": {},
   "source": [
    "Discretizing States:\n",
    "\n",
    "Both the current state and the next_state are discretized using the _discretize_state method. This helps in managing the Q-values in a table format for states that are continuous or too granular.\n",
    "Initialization of Q-values and Eligibility Traces:\n",
    "\n",
    "If the current state or next_state is not in the Q-table (Q), it's added with an initial Q-value of zero for all actions.\n",
    "Similarly, if the current state is not in the eligibility traces table (E), it's added with an initial eligibility trace of zero for all actions.\n",
    "Temporal Difference (TD) Error Calculation:\n",
    "\n",
    "The agent calculates the best action for the next_state based on the current Q-values.\n",
    "The TD error is then computed. It's the difference between the expected Q-value (based on the received reward and the estimated future reward for next_state) and the current Q-value for the taken action at state.\n",
    "Eligibility Trace Update:\n",
    "\n",
    "The eligibility trace for the current state-action pair (state, action) is incremented by 1.\n",
    "This helps in assigning \"credit\" or \"blame\" for this state-action pair for any future reward or punishment. SARSA(λ) updates not just the current state-action pair but also the recent ones based on their eligibility traces.\n",
    "Ensure All States in Q have an Eligibility Trace:\n",
    "\n",
    "The agent makes sure that for all states in the Q-table, there's a corresponding entry in the eligibility traces table. If not, it adds them with a value of zero.\n",
    "Update Q-values Using TD Error and Eligibility Traces:\n",
    "\n",
    "The Q-values for all state-action pairs are updated based on the TD error and their respective eligibility traces. The greater the eligibility trace for a state-action pair, the more it gets updated.\n",
    "This is the heart of the SARSA(λ) algorithm, allowing the Q-value updates to be spread out not just to the current state-action pair but also to the previous ones based on their eligibility.\n",
    "Decay All Eligibility Traces:\n",
    "\n",
    "After updating the Q-values, the eligibility traces for all state-action pairs are decayed by a factor of gamma * td_lambda. This ensures that the traces for older state-action pairs decrease over time, giving more importance to recent state-action pairs for future Q-value updates.\n",
    "In essence, the update method blends the strengths of both SARSA and eligibility traces, providing a more sophisticated update mechanism that takes into account both immediate and more distant state-action pairs when adjusting Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDLambdaAgent:\n",
    "    def __init__(self, epsilon=0.1, epsilon_decay=0.995, alpha=0.1,\n",
    "                  gamma=0.99, td_lambda=lambda_value, n_actions=5,\n",
    "                    no_change_after_lap=150, state_space_discretization=100):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "        self.E = defaultdict(lambda: np.zeros(self.n_actions))  # Eligibility traces\n",
    "        self.n_actions = n_actions\n",
    "        self.state_space_discretization = state_space_discretization\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.no_change_after_lap = no_change_after_lap\n",
    "        self.td_lambda = td_lambda\n",
    "        \n",
    "    def _discretize_state(self, state):\n",
    "        tyre, condition, weather, radius, laps_cleared = state\n",
    "        condition = int(condition * self.state_space_discretization)\n",
    "        \n",
    "        # Discretizing the radius as well by rounding to nearest hundred\n",
    "        radius = round(radius, -2)\n",
    "        return (tyre, condition, weather, radius, laps_cleared)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = self._discretize_state(state)\n",
    "        _, _, _, _, laps_cleared = state\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            if laps_cleared >= self.no_change_after_lap:\n",
    "                return 4  # Don't change tires\n",
    "            return np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.Q.get(state, np.zeros(self.n_actions)))\n",
    "            if laps_cleared >= self.no_change_after_lap and action < 4:\n",
    "                return 4  # Don't change tires\n",
    "            return action\n",
    "        \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        state = self._discretize_state(state)\n",
    "        next_state = self._discretize_state(next_state)\n",
    "        \n",
    "        best_next_action = np.argmax(self.Q[next_state])\n",
    "        td_error = reward + self.gamma * self.Q[next_state][best_next_action] - self.Q[state][action]\n",
    "        \n",
    "        # Increment the eligibility trace for the current state-action pair\n",
    "        self.E[state][action] += 1  \n",
    "        \n",
    "        # Update Q-values for all state-action pairs using TD error and the eligibility traces\n",
    "        for s, actions in self.Q.items():\n",
    "            for a in range(self.n_actions):\n",
    "                self.Q[s][a] += self.alpha * td_error * self.E[s][a]\n",
    "                \n",
    "                # Decay the eligibility trace for the state-action pair\n",
    "                self.E[s][a] *= self.gamma * self.td_lambda\n",
    "\n",
    "# Training the agent\n",
    "agent = TDLambdaAgent()\n",
    "\n",
    "num_episodes = 5\n",
    "eval_episodes = 1\n",
    "\n",
    "episode_rewards = []\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        reward, next_state, done, _ = env.step(action)\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        # state = deepcopy(next_state)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    episode_rewards.append(episode_reward)\n",
    "    agent.epsilon *= agent.epsilon_decay\n",
    "    \n",
    "# Evaluating the agent\n",
    "total_rewards = []\n",
    "for _ in tqdm(range(eval_episodes)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        reward, next_state, done, _ = env.step(action)\n",
    "        # state = deepcopy(next_state)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    total_rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Average reward over {eval_episodes} episodes: {np.mean(total_rewards)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5\n",
    "eval_episodes = 1\n",
    "\n",
    "# Grid Search\n",
    "\"\"\"\n",
    "param_grid: dict\n",
    "    A dictionary containing hyperparameters and their possible values to be explored during grid search. Each key represents a hyperparameter, and its corresponding value is a list of values that will be tested for that hyperparameter.\n",
    "\n",
    "    Parameters:\n",
    "    - epsilon: list of floats\n",
    "        The initial exploration rate for the epsilon-greedy policy. Values represent the probability at which the agent will explore, i.e., take random actions. Higher values mean more exploration and less exploitation.\n",
    "\n",
    "    - epsilon_decay: list of floats\n",
    "        The rate at which epsilon will be reduced after each episode. Values are in the range (0, 1) with values closer to 1 meaning a slower decay of epsilon over time.\n",
    "\n",
    "    - alpha: list of floats\n",
    "        The learning rate for the Q-learning algorithm. Determines to what extent newly acquired information overrides old information. A value of 0 would make the agent not learn anything, while a value of 1 would make the agent consider only the most recent information.\n",
    "\n",
    "    - gamma: list of floats\n",
    "        The discount factor for the Q-learning algorithm. Represents the agent's consideration for future rewards. A value of 0 makes the agent short-sighted by only considering current rewards, while a value close to 1 will make it aim for a long-term high reward.\n",
    "\n",
    "    - no_change_after_lap: list of integers\n",
    "        Specifies the lap number after which the agent will not change tires. If the current lap exceeds this number, actions to change tires will not be taken.\n",
    "\"\"\"\n",
    "param_grid = {\n",
    "    'epsilon': [0.0, 0.025, 0.05, 0.1, 0.2],\n",
    "    'epsilon_decay': [0.990, 0.995, 0.999],\n",
    "    'alpha': [0.0, 0.2, 0.4, 0.6, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0.0, 0.2, 0.4, 0.6, 0.8, 0.9, 1.0],\n",
    "    'no_change_after_lap': [x for x in range(100, 161, 5)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_database():\n",
    "    conn = sqlite3.connect(gs_db_name)\n",
    "    c = conn.cursor()\n",
    "    c.execute(f'''CREATE TABLE IF NOT EXISTS {table_name}\n",
    "                 (epsilon real, epsilon_decay real, alpha real, gamma real, no_change_after_lap integer, \n",
    "                  avg_last_50 real, min_last_50 real, max_last_50 real, \n",
    "                  eval_avg real, overall_avg real)''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "setup_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3661e739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [11:36, 15.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18216.49\n",
      "    Min Reward over last 50 episodes: -18248.62\n",
      "    Max Reward over last 50 episodes: -18184.36\n",
      "    Evaluation Average Reward: -17908.78\n",
      "\n",
      "Running combination 47/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.2, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [11:51, 15.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16341.76\n",
      "    Min Reward over last 50 episodes: -16432.36\n",
      "    Max Reward over last 50 episodes: -16251.17\n",
      "    Evaluation Average Reward: -16414.86\n",
      "\n",
      "Running combination 48/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 1.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [12:06, 15.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16102.28\n",
      "    Min Reward over last 50 episodes: -16105.15\n",
      "    Max Reward over last 50 episodes: -16099.42\n",
      "    Evaluation Average Reward: -16088.13\n",
      "\n",
      "Running combination 49/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 1.0, 'gamma': 1.0, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [12:21, 15.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17019.77\n",
      "    Min Reward over last 50 episodes: -17027.81\n",
      "    Max Reward over last 50 episodes: -17011.72\n",
      "    Evaluation Average Reward: -16897.15\n",
      "\n",
      "Running combination 50/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.4, 'gamma': 0.6, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [12:36, 15.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17852.16\n",
      "    Min Reward over last 50 episodes: -17854.75\n",
      "    Max Reward over last 50 episodes: -17849.57\n",
      "    Evaluation Average Reward: -17973.63\n",
      "\n",
      "Running combination 51/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.8, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [12:51, 15.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18104.14\n",
      "    Min Reward over last 50 episodes: -18326.98\n",
      "    Max Reward over last 50 episodes: -17881.30\n",
      "    Evaluation Average Reward: -17852.36\n",
      "\n",
      "Running combination 52/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.2, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [13:09, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26876.54\n",
      "    Min Reward over last 50 episodes: -27122.52\n",
      "    Max Reward over last 50 episodes: -26630.57\n",
      "    Evaluation Average Reward: -26847.82\n",
      "\n",
      "Running combination 53/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.2, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [13:26, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29198.66\n",
      "    Min Reward over last 50 episodes: -29409.94\n",
      "    Max Reward over last 50 episodes: -28987.38\n",
      "    Evaluation Average Reward: -29339.09\n",
      "\n",
      "Running combination 54/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 1.0, 'gamma': 0.8, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [13:43, 16.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17250.79\n",
      "    Min Reward over last 50 episodes: -17271.64\n",
      "    Max Reward over last 50 episodes: -17229.95\n",
      "    Evaluation Average Reward: -16997.30\n",
      "\n",
      "Running combination 55/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.9, 'gamma': 0.9, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [14:00, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23087.51\n",
      "    Min Reward over last 50 episodes: -23330.25\n",
      "    Max Reward over last 50 episodes: -22844.76\n",
      "    Evaluation Average Reward: -22779.80\n",
      "\n",
      "Running combination 56/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.8, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [14:16, 16.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29309.89\n",
      "    Min Reward over last 50 episodes: -29414.93\n",
      "    Max Reward over last 50 episodes: -29204.85\n",
      "    Evaluation Average Reward: -29224.59\n",
      "\n",
      "Running combination 57/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.4, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [14:31, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16073.60\n",
      "    Min Reward over last 50 episodes: -16120.72\n",
      "    Max Reward over last 50 episodes: -16026.49\n",
      "    Evaluation Average Reward: -16082.88\n",
      "\n",
      "Running combination 58/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.4, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [14:46, 15.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19329.46\n",
      "    Min Reward over last 50 episodes: -19513.65\n",
      "    Max Reward over last 50 episodes: -19145.28\n",
      "    Evaluation Average Reward: -19034.58\n",
      "\n",
      "Running combination 59/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.9, 'gamma': 1.0, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [15:01, 15.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28006.51\n",
      "    Min Reward over last 50 episodes: -28112.54\n",
      "    Max Reward over last 50 episodes: -27900.48\n",
      "    Evaluation Average Reward: -28112.33\n",
      "\n",
      "Running combination 60/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.2, 'gamma': 0.9, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [15:17, 15.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16313.40\n",
      "    Min Reward over last 50 episodes: -16344.67\n",
      "    Max Reward over last 50 episodes: -16282.14\n",
      "    Evaluation Average Reward: -16188.56\n",
      "\n",
      "Running combination 61/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.4, 'gamma': 0.2, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [15:32, 15.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16373.34\n",
      "    Min Reward over last 50 episodes: -16420.07\n",
      "    Max Reward over last 50 episodes: -16326.61\n",
      "    Evaluation Average Reward: -16443.72\n",
      "\n",
      "Running combination 62/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.9, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [15:49, 15.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19372.08\n",
      "    Min Reward over last 50 episodes: -19429.90\n",
      "    Max Reward over last 50 episodes: -19314.27\n",
      "    Evaluation Average Reward: -19015.41\n",
      "\n",
      "Running combination 63/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.9, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [16:06, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16999.43\n",
      "    Min Reward over last 50 episodes: -17044.46\n",
      "    Max Reward over last 50 episodes: -16954.40\n",
      "    Evaluation Average Reward: -17091.85\n",
      "\n",
      "Running combination 64/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 1.0, 'gamma': 1.0, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [16:22, 16.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17179.90\n",
      "    Min Reward over last 50 episodes: -17235.33\n",
      "    Max Reward over last 50 episodes: -17124.48\n",
      "    Evaluation Average Reward: -17183.61\n",
      "\n",
      "Running combination 65/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.8, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [16:37, 15.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16258.09\n",
      "    Min Reward over last 50 episodes: -16304.49\n",
      "    Max Reward over last 50 episodes: -16211.69\n",
      "    Evaluation Average Reward: -16283.58\n",
      "\n",
      "Running combination 66/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.2, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [16:53, 15.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21867.59\n",
      "    Min Reward over last 50 episodes: -21984.36\n",
      "    Max Reward over last 50 episodes: -21750.83\n",
      "    Evaluation Average Reward: -21868.72\n",
      "\n",
      "Running combination 67/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.9, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [17:08, 15.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29358.76\n",
      "    Min Reward over last 50 episodes: -29415.44\n",
      "    Max Reward over last 50 episodes: -29302.08\n",
      "    Evaluation Average Reward: -29635.22\n",
      "\n",
      "Running combination 68/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.2, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [17:23, 15.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24379.52\n",
      "    Min Reward over last 50 episodes: -24431.08\n",
      "    Max Reward over last 50 episodes: -24327.96\n",
      "    Evaluation Average Reward: -24628.90\n",
      "\n",
      "Running combination 69/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.0, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [17:39, 15.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21742.49\n",
      "    Min Reward over last 50 episodes: -21980.23\n",
      "    Max Reward over last 50 episodes: -21504.75\n",
      "    Evaluation Average Reward: -21436.86\n",
      "\n",
      "Running combination 70/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 1.0, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [17:53, 15.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21852.32\n",
      "    Min Reward over last 50 episodes: -21950.38\n",
      "    Max Reward over last 50 episodes: -21754.25\n",
      "    Evaluation Average Reward: -21572.78\n",
      "\n",
      "Running combination 71/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.9, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [18:09, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16007.03\n",
      "    Min Reward over last 50 episodes: -16024.08\n",
      "    Max Reward over last 50 episodes: -15989.97\n",
      "    Evaluation Average Reward: -16071.58\n",
      "\n",
      "Running combination 72/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.0, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [18:23, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20497.52\n",
      "    Min Reward over last 50 episodes: -20737.51\n",
      "    Max Reward over last 50 episodes: -20257.53\n",
      "    Evaluation Average Reward: -20544.08\n",
      "\n",
      "Running combination 73/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.4, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [18:38, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16338.75\n",
      "    Min Reward over last 50 episodes: -16428.01\n",
      "    Max Reward over last 50 episodes: -16249.50\n",
      "    Evaluation Average Reward: -16398.81\n",
      "\n",
      "Running combination 74/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.2, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [18:53, 14.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24605.12\n",
      "    Min Reward over last 50 episodes: -24650.92\n",
      "    Max Reward over last 50 episodes: -24559.32\n",
      "    Evaluation Average Reward: -24309.61\n",
      "\n",
      "Running combination 75/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.4, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [19:08, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21752.35\n",
      "    Min Reward over last 50 episodes: -21849.37\n",
      "    Max Reward over last 50 episodes: -21655.33\n",
      "    Evaluation Average Reward: -21608.94\n",
      "\n",
      "Running combination 76/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.2, 'gamma': 0.2, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [19:23, 15.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23055.12\n",
      "    Min Reward over last 50 episodes: -23143.07\n",
      "    Max Reward over last 50 episodes: -22967.18\n",
      "    Evaluation Average Reward: -23370.43\n",
      "\n",
      "Running combination 77/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.9, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [19:39, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -22065.52\n",
      "    Min Reward over last 50 episodes: -22151.99\n",
      "    Max Reward over last 50 episodes: -21979.05\n",
      "    Evaluation Average Reward: -21939.74\n",
      "\n",
      "Running combination 78/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.6, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [19:54, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29482.45\n",
      "    Min Reward over last 50 episodes: -29604.93\n",
      "    Max Reward over last 50 episodes: -29359.97\n",
      "    Evaluation Average Reward: -29553.93\n",
      "\n",
      "Running combination 79/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.9, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [20:09, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16989.07\n",
      "    Min Reward over last 50 episodes: -17039.47\n",
      "    Max Reward over last 50 episodes: -16938.68\n",
      "    Evaluation Average Reward: -17243.29\n",
      "\n",
      "Running combination 80/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.9, 'gamma': 0.6, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [20:24, 15.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26692.62\n",
      "    Min Reward over last 50 episodes: -26695.77\n",
      "    Max Reward over last 50 episodes: -26689.48\n",
      "    Evaluation Average Reward: -26688.74\n",
      "\n",
      "Running combination 81/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.2, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [20:39, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24287.42\n",
      "    Min Reward over last 50 episodes: -24486.78\n",
      "    Max Reward over last 50 episodes: -24088.06\n",
      "    Evaluation Average Reward: -24335.42\n",
      "\n",
      "Running combination 82/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.0, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82it [20:54, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21940.41\n",
      "    Min Reward over last 50 episodes: -21973.07\n",
      "    Max Reward over last 50 episodes: -21907.75\n",
      "    Evaluation Average Reward: -21566.11\n",
      "\n",
      "Running combination 83/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.0, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [21:09, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28093.57\n",
      "    Min Reward over last 50 episodes: -28160.99\n",
      "    Max Reward over last 50 episodes: -28026.14\n",
      "    Evaluation Average Reward: -27956.77\n",
      "\n",
      "Running combination 84/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.6, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [21:23, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -22102.50\n",
      "    Min Reward over last 50 episodes: -22136.88\n",
      "    Max Reward over last 50 episodes: -22068.11\n",
      "    Evaluation Average Reward: -21996.84\n",
      "\n",
      "Running combination 85/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 1.0, 'gamma': 0.0, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [21:37, 14.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24563.95\n",
      "    Min Reward over last 50 episodes: -24637.85\n",
      "    Max Reward over last 50 episodes: -24490.06\n",
      "    Evaluation Average Reward: -24173.19\n",
      "\n",
      "Running combination 86/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.9, 'gamma': 0.4, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [21:52, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16080.35\n",
      "    Min Reward over last 50 episodes: -16131.39\n",
      "    Max Reward over last 50 episodes: -16029.32\n",
      "    Evaluation Average Reward: -16101.13\n",
      "\n",
      "Running combination 87/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 0.2, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87it [22:07, 14.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28159.17\n",
      "    Min Reward over last 50 episodes: -28238.59\n",
      "    Max Reward over last 50 episodes: -28079.74\n",
      "    Evaluation Average Reward: -28310.28\n",
      "\n",
      "Running combination 88/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.8, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [22:22, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16056.67\n",
      "    Min Reward over last 50 episodes: -16083.75\n",
      "    Max Reward over last 50 episodes: -16029.60\n",
      "    Evaluation Average Reward: -16099.41\n",
      "\n",
      "Running combination 89/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 1.0, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [22:37, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28209.09\n",
      "    Min Reward over last 50 episodes: -28464.39\n",
      "    Max Reward over last 50 episodes: -27953.79\n",
      "    Evaluation Average Reward: -28373.08\n",
      "\n",
      "Running combination 90/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.6, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [22:51, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18412.79\n",
      "    Min Reward over last 50 episodes: -18449.44\n",
      "    Max Reward over last 50 episodes: -18376.14\n",
      "    Evaluation Average Reward: -18398.00\n",
      "\n",
      "Running combination 91/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.2, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [23:06, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16068.54\n",
      "    Min Reward over last 50 episodes: -16127.47\n",
      "    Max Reward over last 50 episodes: -16009.62\n",
      "    Evaluation Average Reward: -16103.70\n",
      "\n",
      "Running combination 92/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.0, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [23:21, 14.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24251.00\n",
      "    Min Reward over last 50 episodes: -24472.24\n",
      "    Max Reward over last 50 episodes: -24029.76\n",
      "    Evaluation Average Reward: -24088.60\n",
      "\n",
      "Running combination 93/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.9, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93it [23:36, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26889.70\n",
      "    Min Reward over last 50 episodes: -27133.66\n",
      "    Max Reward over last 50 episodes: -26645.74\n",
      "    Evaluation Average Reward: -26991.32\n",
      "\n",
      "Running combination 94/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.2, 'gamma': 0.2, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [23:51, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -22991.30\n",
      "    Min Reward over last 50 episodes: -23152.72\n",
      "    Max Reward over last 50 episodes: -22829.88\n",
      "    Evaluation Average Reward: -22880.96\n",
      "\n",
      "Running combination 95/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 1.0, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [24:06, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21927.73\n",
      "    Min Reward over last 50 episodes: -22075.33\n",
      "    Max Reward over last 50 episodes: -21780.13\n",
      "    Evaluation Average Reward: -22077.22\n",
      "\n",
      "Running combination 96/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.0, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [24:21, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28034.09\n",
      "    Min Reward over last 50 episodes: -28050.98\n",
      "    Max Reward over last 50 episodes: -28017.20\n",
      "    Evaluation Average Reward: -27830.82\n",
      "\n",
      "Running combination 97/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.9, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "97it [24:36, 14.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -22925.29\n",
      "    Min Reward over last 50 episodes: -23071.41\n",
      "    Max Reward over last 50 episodes: -22779.17\n",
      "    Evaluation Average Reward: -23369.49\n",
      "\n",
      "Running combination 98/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.4, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [24:51, 15.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16329.42\n",
      "    Min Reward over last 50 episodes: -16390.88\n",
      "    Max Reward over last 50 episodes: -16267.95\n",
      "    Evaluation Average Reward: -16403.12\n",
      "\n",
      "Running combination 99/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.2, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [25:06, 14.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25819.07\n",
      "    Min Reward over last 50 episodes: -25829.01\n",
      "    Max Reward over last 50 episodes: -25809.12\n",
      "    Evaluation Average Reward: -25760.87\n",
      "\n",
      "Running combination 100/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.9, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [25:21, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25491.90\n",
      "    Min Reward over last 50 episodes: -25757.40\n",
      "    Max Reward over last 50 episodes: -25226.40\n",
      "    Evaluation Average Reward: -25460.20\n",
      "\n",
      "Running combination 101/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.6, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [25:36, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24390.48\n",
      "    Min Reward over last 50 episodes: -24463.81\n",
      "    Max Reward over last 50 episodes: -24317.14\n",
      "    Evaluation Average Reward: -24645.05\n",
      "\n",
      "Running combination 102/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 0.4, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [25:51, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19303.28\n",
      "    Min Reward over last 50 episodes: -19328.41\n",
      "    Max Reward over last 50 episodes: -19278.15\n",
      "    Evaluation Average Reward: -19377.38\n",
      "\n",
      "Running combination 103/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.9, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "103it [26:05, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25915.59\n",
      "    Min Reward over last 50 episodes: -25961.73\n",
      "    Max Reward over last 50 episodes: -25869.46\n",
      "    Evaluation Average Reward: -25718.24\n",
      "\n",
      "Running combination 104/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 1.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104it [26:20, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16097.96\n",
      "    Min Reward over last 50 episodes: -16118.64\n",
      "    Max Reward over last 50 episodes: -16077.28\n",
      "    Evaluation Average Reward: -16006.02\n",
      "\n",
      "Running combination 105/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [26:35, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16097.52\n",
      "    Min Reward over last 50 episodes: -16123.56\n",
      "    Max Reward over last 50 episodes: -16071.48\n",
      "    Evaluation Average Reward: -16066.32\n",
      "\n",
      "Running combination 106/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.2, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106it [26:50, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24385.37\n",
      "    Min Reward over last 50 episodes: -24496.13\n",
      "    Max Reward over last 50 episodes: -24274.62\n",
      "    Evaluation Average Reward: -24268.96\n",
      "\n",
      "Running combination 107/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.8, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "107it [27:05, 14.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19477.03\n",
      "    Min Reward over last 50 episodes: -19631.57\n",
      "    Max Reward over last 50 episodes: -19322.50\n",
      "    Evaluation Average Reward: -19528.77\n",
      "\n",
      "Running combination 108/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.4, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [27:19, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18122.37\n",
      "    Min Reward over last 50 episodes: -18162.91\n",
      "    Max Reward over last 50 episodes: -18081.84\n",
      "    Evaluation Average Reward: -18370.57\n",
      "\n",
      "Running combination 109/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.6, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109it [27:34, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23214.72\n",
      "    Min Reward over last 50 episodes: -23263.91\n",
      "    Max Reward over last 50 episodes: -23165.53\n",
      "    Evaluation Average Reward: -23226.64\n",
      "\n",
      "Running combination 110/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 1.0, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [27:48, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16316.26\n",
      "    Min Reward over last 50 episodes: -16383.46\n",
      "    Max Reward over last 50 episodes: -16249.06\n",
      "    Evaluation Average Reward: -16366.84\n",
      "\n",
      "Running combination 111/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.8, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [28:03, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23278.28\n",
      "    Min Reward over last 50 episodes: -23279.38\n",
      "    Max Reward over last 50 episodes: -23277.18\n",
      "    Evaluation Average Reward: -23016.24\n",
      "\n",
      "Running combination 112/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.9, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [28:18, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28254.57\n",
      "    Min Reward over last 50 episodes: -28269.62\n",
      "    Max Reward over last 50 episodes: -28239.53\n",
      "    Evaluation Average Reward: -28009.53\n",
      "\n",
      "Running combination 113/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 1.0, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [28:34, 14.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23123.34\n",
      "    Min Reward over last 50 episodes: -23313.20\n",
      "    Max Reward over last 50 episodes: -22933.49\n",
      "    Evaluation Average Reward: -22796.15\n",
      "\n",
      "Running combination 114/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.6, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114it [28:48, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25768.70\n",
      "    Min Reward over last 50 episodes: -25812.75\n",
      "    Max Reward over last 50 episodes: -25724.66\n",
      "    Evaluation Average Reward: -25486.45\n",
      "\n",
      "Running combination 115/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.8, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115it [29:03, 14.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21759.16\n",
      "    Min Reward over last 50 episodes: -22013.93\n",
      "    Max Reward over last 50 episodes: -21504.40\n",
      "    Evaluation Average Reward: -21885.82\n",
      "\n",
      "Running combination 116/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.8, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116it [29:17, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18192.30\n",
      "    Min Reward over last 50 episodes: -18413.62\n",
      "    Max Reward over last 50 episodes: -17970.98\n",
      "    Evaluation Average Reward: -17902.23\n",
      "\n",
      "Running combination 117/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.6, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117it [29:32, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16284.86\n",
      "    Min Reward over last 50 episodes: -16313.80\n",
      "    Max Reward over last 50 episodes: -16255.93\n",
      "    Evaluation Average Reward: -16446.67\n",
      "\n",
      "Running combination 118/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.8, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118it [29:47, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18056.16\n",
      "    Min Reward over last 50 episodes: -18233.78\n",
      "    Max Reward over last 50 episodes: -17878.55\n",
      "    Evaluation Average Reward: -17822.76\n",
      "\n",
      "Running combination 119/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 1.0, 'gamma': 0.0, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "119it [30:01, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16392.61\n",
      "    Min Reward over last 50 episodes: -16427.66\n",
      "    Max Reward over last 50 episodes: -16357.56\n",
      "    Evaluation Average Reward: -16327.99\n",
      "\n",
      "Running combination 120/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.2, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [30:16, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19353.49\n",
      "    Min Reward over last 50 episodes: -19583.36\n",
      "    Max Reward over last 50 episodes: -19123.63\n",
      "    Evaluation Average Reward: -19069.95\n",
      "\n",
      "Running combination 121/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.2, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [30:31, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17104.68\n",
      "    Min Reward over last 50 episodes: -17296.14\n",
      "    Max Reward over last 50 episodes: -16913.22\n",
      "    Evaluation Average Reward: -17251.25\n",
      "\n",
      "Running combination 122/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 0.0, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "122it [30:46, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25671.25\n",
      "    Min Reward over last 50 episodes: -25724.65\n",
      "    Max Reward over last 50 episodes: -25617.85\n",
      "    Evaluation Average Reward: -25726.49\n",
      "\n",
      "Running combination 123/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.0, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [31:00, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25745.11\n",
      "    Min Reward over last 50 episodes: -25829.12\n",
      "    Max Reward over last 50 episodes: -25661.11\n",
      "    Evaluation Average Reward: -25272.31\n",
      "\n",
      "Running combination 124/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.8, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124it [31:15, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18054.12\n",
      "    Min Reward over last 50 episodes: -18210.28\n",
      "    Max Reward over last 50 episodes: -17897.97\n",
      "    Evaluation Average Reward: -18001.50\n",
      "\n",
      "Running combination 125/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 0.8, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [31:30, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -22105.44\n",
      "    Min Reward over last 50 episodes: -22141.41\n",
      "    Max Reward over last 50 episodes: -22069.46\n",
      "    Evaluation Average Reward: -21869.80\n",
      "\n",
      "Running combination 126/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.9, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126it [31:45, 14.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16301.05\n",
      "    Min Reward over last 50 episodes: -16305.44\n",
      "    Max Reward over last 50 episodes: -16296.67\n",
      "    Evaluation Average Reward: -16368.43\n",
      "\n",
      "Running combination 127/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 1.0, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [32:00, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -27814.95\n",
      "    Min Reward over last 50 episodes: -27927.24\n",
      "    Max Reward over last 50 episodes: -27702.65\n",
      "    Evaluation Average Reward: -27912.92\n",
      "\n",
      "Running combination 128/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.4, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128it [32:15, 14.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16377.91\n",
      "    Min Reward over last 50 episodes: -16406.24\n",
      "    Max Reward over last 50 episodes: -16349.58\n",
      "    Evaluation Average Reward: -16297.61\n",
      "\n",
      "Running combination 129/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.9, 'gamma': 0.8, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129it [32:30, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19193.15\n",
      "    Min Reward over last 50 episodes: -19205.52\n",
      "    Max Reward over last 50 episodes: -19180.77\n",
      "    Evaluation Average Reward: -19589.44\n",
      "\n",
      "Running combination 130/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.2, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [32:45, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16309.74\n",
      "    Min Reward over last 50 episodes: -16396.07\n",
      "    Max Reward over last 50 episodes: -16223.40\n",
      "    Evaluation Average Reward: -16419.44\n",
      "\n",
      "Running combination 131/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.9, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "131it [33:00, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29291.31\n",
      "    Min Reward over last 50 episodes: -29459.80\n",
      "    Max Reward over last 50 episodes: -29122.82\n",
      "    Evaluation Average Reward: -29251.56\n",
      "\n",
      "Running combination 132/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.4, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "132it [33:15, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21779.12\n",
      "    Min Reward over last 50 episodes: -22077.50\n",
      "    Max Reward over last 50 episodes: -21480.75\n",
      "    Evaluation Average Reward: -21559.75\n",
      "\n",
      "Running combination 133/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 1.0, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "133it [33:30, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19353.66\n",
      "    Min Reward over last 50 episodes: -19385.63\n",
      "    Max Reward over last 50 episodes: -19321.70\n",
      "    Evaluation Average Reward: -19587.19\n",
      "\n",
      "Running combination 134/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.8, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "134it [33:45, 14.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29529.66\n",
      "    Min Reward over last 50 episodes: -29647.27\n",
      "    Max Reward over last 50 episodes: -29412.06\n",
      "    Evaluation Average Reward: -29379.45\n",
      "\n",
      "Running combination 135/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.4, 'gamma': 0.0, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "135it [34:00, 14.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26983.33\n",
      "    Min Reward over last 50 episodes: -27114.19\n",
      "    Max Reward over last 50 episodes: -26852.47\n",
      "    Evaluation Average Reward: -26520.72\n",
      "\n",
      "Running combination 136/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.2, 'gamma': 0.6, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "136it [34:15, 14.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25641.94\n",
      "    Min Reward over last 50 episodes: -25779.32\n",
      "    Max Reward over last 50 episodes: -25504.55\n",
      "    Evaluation Average Reward: -25449.92\n",
      "\n",
      "Running combination 137/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.0, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "137it [34:30, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17090.01\n",
      "    Min Reward over last 50 episodes: -17283.57\n",
      "    Max Reward over last 50 episodes: -16896.44\n",
      "    Evaluation Average Reward: -17000.80\n",
      "\n",
      "Running combination 138/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 1.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138it [34:44, 14.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16065.92\n",
      "    Min Reward over last 50 episodes: -16113.00\n",
      "    Max Reward over last 50 episodes: -16018.83\n",
      "    Evaluation Average Reward: -16047.32\n",
      "\n",
      "Running combination 139/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 0.8, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "139it [34:59, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17925.55\n",
      "    Min Reward over last 50 episodes: -17995.30\n",
      "    Max Reward over last 50 episodes: -17855.79\n",
      "    Evaluation Average Reward: -18146.85\n",
      "\n",
      "Running combination 140/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 0.6, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [35:14, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19417.12\n",
      "    Min Reward over last 50 episodes: -19591.94\n",
      "    Max Reward over last 50 episodes: -19242.31\n",
      "    Evaluation Average Reward: -19319.24\n",
      "\n",
      "Running combination 141/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 1.0, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141it [35:28, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16283.90\n",
      "    Min Reward over last 50 episodes: -16341.97\n",
      "    Max Reward over last 50 episodes: -16225.82\n",
      "    Evaluation Average Reward: -16256.94\n",
      "\n",
      "Running combination 142/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 0.0, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "142it [35:43, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18255.75\n",
      "    Min Reward over last 50 episodes: -18306.56\n",
      "    Max Reward over last 50 episodes: -18204.94\n",
      "    Evaluation Average Reward: -18323.05\n",
      "\n",
      "Running combination 143/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.9, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "143it [35:58, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26801.13\n",
      "    Min Reward over last 50 episodes: -27154.91\n",
      "    Max Reward over last 50 episodes: -26447.35\n",
      "    Evaluation Average Reward: -26612.05\n",
      "SKIPPING\n",
      "\n",
      "Running combination 144/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.4, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [36:13, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29221.86\n",
      "    Min Reward over last 50 episodes: -29400.24\n",
      "    Max Reward over last 50 episodes: -29043.48\n",
      "    Evaluation Average Reward: -29182.63\n",
      "SKIPPING\n",
      "\n",
      "Running combination 145/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.6, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [36:28, 14.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -22077.49\n",
      "    Min Reward over last 50 episodes: -22090.43\n",
      "    Max Reward over last 50 episodes: -22064.56\n",
      "    Evaluation Average Reward: -21962.22\n",
      "\n",
      "Running combination 146/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.4, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "146it [36:43, 14.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25658.71\n",
      "    Min Reward over last 50 episodes: -25820.06\n",
      "    Max Reward over last 50 episodes: -25497.36\n",
      "    Evaluation Average Reward: -25815.01\n",
      "\n",
      "Running combination 147/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 1.0, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "147it [36:58, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25602.64\n",
      "    Min Reward over last 50 episodes: -25834.15\n",
      "    Max Reward over last 50 episodes: -25371.12\n",
      "    Evaluation Average Reward: -25814.09\n",
      "\n",
      "Running combination 148/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.4, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [37:12, 14.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -22045.03\n",
      "    Min Reward over last 50 episodes: -22116.18\n",
      "    Max Reward over last 50 episodes: -21973.88\n",
      "    Evaluation Average Reward: -21655.43\n",
      "\n",
      "Running combination 149/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.9, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "149it [37:27, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -22027.45\n",
      "    Min Reward over last 50 episodes: -22115.43\n",
      "    Max Reward over last 50 episodes: -21939.48\n",
      "    Evaluation Average Reward: -21817.28\n",
      "\n",
      "Running combination 150/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.0, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [37:42, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26812.99\n",
      "    Min Reward over last 50 episodes: -27067.95\n",
      "    Max Reward over last 50 episodes: -26558.02\n",
      "    Evaluation Average Reward: -27022.27\n",
      "\n",
      "Running combination 151/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.9, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "151it [37:57, 14.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17194.43\n",
      "    Min Reward over last 50 episodes: -17260.83\n",
      "    Max Reward over last 50 episodes: -17128.03\n",
      "    Evaluation Average Reward: -17105.94\n",
      "\n",
      "Running combination 152/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 0.6, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [38:12, 14.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18341.24\n",
      "    Min Reward over last 50 episodes: -18354.17\n",
      "    Max Reward over last 50 episodes: -18328.31\n",
      "    Evaluation Average Reward: -18214.87\n",
      "\n",
      "Running combination 153/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.6, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "153it [38:26, 14.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29444.25\n",
      "    Min Reward over last 50 episodes: -29615.80\n",
      "    Max Reward over last 50 episodes: -29272.70\n",
      "    Evaluation Average Reward: -28997.29\n",
      "\n",
      "Running combination 154/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.6, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "154it [38:41, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24245.34\n",
      "    Min Reward over last 50 episodes: -24359.28\n",
      "    Max Reward over last 50 episodes: -24131.41\n",
      "    Evaluation Average Reward: -24228.21\n",
      "\n",
      "Running combination 155/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.6, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "155it [38:55, 14.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24371.83\n",
      "    Min Reward over last 50 episodes: -24415.01\n",
      "    Max Reward over last 50 episodes: -24328.66\n",
      "    Evaluation Average Reward: -24464.49\n",
      "\n",
      "Running combination 156/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 1.0, 'gamma': 0.4, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [39:10, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25711.40\n",
      "    Min Reward over last 50 episodes: -25755.15\n",
      "    Max Reward over last 50 episodes: -25667.65\n",
      "    Evaluation Average Reward: -25416.68\n",
      "\n",
      "Running combination 157/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.9, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [39:25, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20596.47\n",
      "    Min Reward over last 50 episodes: -20725.66\n",
      "    Max Reward over last 50 episodes: -20467.28\n",
      "    Evaluation Average Reward: -20858.77\n",
      "\n",
      "Running combination 158/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.0, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "158it [39:40, 14.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28166.40\n",
      "    Min Reward over last 50 episodes: -28189.52\n",
      "    Max Reward over last 50 episodes: -28143.29\n",
      "    Evaluation Average Reward: -27761.09\n",
      "\n",
      "Running combination 159/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.4, 'gamma': 0.4, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159it [39:55, 14.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23226.26\n",
      "    Min Reward over last 50 episodes: -23368.90\n",
      "    Max Reward over last 50 episodes: -23083.62\n",
      "    Evaluation Average Reward: -22780.42\n",
      "\n",
      "Running combination 160/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 1.0, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [40:09, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23140.31\n",
      "    Min Reward over last 50 episodes: -23334.39\n",
      "    Max Reward over last 50 episodes: -22946.23\n",
      "    Evaluation Average Reward: -23294.79\n",
      "\n",
      "Running combination 161/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.4, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [40:24, 14.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -27906.97\n",
      "    Min Reward over last 50 episodes: -28022.34\n",
      "    Max Reward over last 50 episodes: -27791.61\n",
      "    Evaluation Average Reward: -28432.36\n",
      "\n",
      "Running combination 162/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 0.0, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "162it [40:39, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29166.06\n",
      "    Min Reward over last 50 episodes: -29234.29\n",
      "    Max Reward over last 50 episodes: -29097.84\n",
      "    Evaluation Average Reward: -29543.74\n",
      "\n",
      "Running combination 163/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.0, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "163it [40:54, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25716.88\n",
      "    Min Reward over last 50 episodes: -25739.05\n",
      "    Max Reward over last 50 episodes: -25694.71\n",
      "    Evaluation Average Reward: -25481.75\n",
      "\n",
      "Running combination 164/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [41:08, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16094.68\n",
      "    Min Reward over last 50 episodes: -16155.36\n",
      "    Max Reward over last 50 episodes: -16034.01\n",
      "    Evaluation Average Reward: -16069.86\n",
      "\n",
      "Running combination 165/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.4, 'gamma': 1.0, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "165it [41:23, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23132.97\n",
      "    Min Reward over last 50 episodes: -23347.89\n",
      "    Max Reward over last 50 episodes: -22918.06\n",
      "    Evaluation Average Reward: -23382.80\n",
      "\n",
      "Running combination 166/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 0.9, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [41:39, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16370.09\n",
      "    Min Reward over last 50 episodes: -16444.14\n",
      "    Max Reward over last 50 episodes: -16296.05\n",
      "    Evaluation Average Reward: -16307.09\n",
      "\n",
      "Running combination 167/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.2, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [41:53, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18262.78\n",
      "    Min Reward over last 50 episodes: -18425.68\n",
      "    Max Reward over last 50 episodes: -18099.89\n",
      "    Evaluation Average Reward: -18221.07\n",
      "\n",
      "Running combination 168/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 0.9, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168it [42:08, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -27030.01\n",
      "    Min Reward over last 50 episodes: -27068.72\n",
      "    Max Reward over last 50 episodes: -26991.30\n",
      "    Evaluation Average Reward: -27123.80\n",
      "\n",
      "Running combination 169/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.9, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169it [42:24, 14.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20422.94\n",
      "    Min Reward over last 50 episodes: -20444.38\n",
      "    Max Reward over last 50 episodes: -20401.51\n",
      "    Evaluation Average Reward: -20653.25\n",
      "\n",
      "Running combination 170/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.9, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170it [42:38, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19492.40\n",
      "    Min Reward over last 50 episodes: -19644.81\n",
      "    Max Reward over last 50 episodes: -19339.98\n",
      "    Evaluation Average Reward: -19383.85\n",
      "\n",
      "Running combination 171/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 1.0, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "171it [42:54, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24271.20\n",
      "    Min Reward over last 50 episodes: -24432.82\n",
      "    Max Reward over last 50 episodes: -24109.59\n",
      "    Evaluation Average Reward: -24420.29\n",
      "\n",
      "Running combination 172/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.9, 'gamma': 1.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172it [43:08, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -15985.65\n",
      "    Min Reward over last 50 episodes: -16010.50\n",
      "    Max Reward over last 50 episodes: -15960.80\n",
      "    Evaluation Average Reward: -16074.80\n",
      "\n",
      "Running combination 173/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 1.0, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [43:24, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23094.79\n",
      "    Min Reward over last 50 episodes: -23275.66\n",
      "    Max Reward over last 50 episodes: -22913.92\n",
      "    Evaluation Average Reward: -22948.66\n",
      "\n",
      "Running combination 174/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.2, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "174it [43:38, 14.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29417.11\n",
      "    Min Reward over last 50 episodes: -29421.74\n",
      "    Max Reward over last 50 episodes: -29412.49\n",
      "    Evaluation Average Reward: -29431.85\n",
      "\n",
      "Running combination 175/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 1.0, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "175it [43:53, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24312.08\n",
      "    Min Reward over last 50 episodes: -24320.10\n",
      "    Max Reward over last 50 episodes: -24304.06\n",
      "    Evaluation Average Reward: -24471.39\n",
      "\n",
      "Running combination 176/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.0, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [44:08, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19310.27\n",
      "    Min Reward over last 50 episodes: -19508.72\n",
      "    Max Reward over last 50 episodes: -19111.83\n",
      "    Evaluation Average Reward: -19021.51\n",
      "\n",
      "Running combination 177/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.2, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "177it [44:23, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28114.72\n",
      "    Min Reward over last 50 episodes: -28350.44\n",
      "    Max Reward over last 50 episodes: -27879.00\n",
      "    Evaluation Average Reward: -28281.96\n",
      "\n",
      "Running combination 178/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.2, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178it [44:38, 14.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16101.45\n",
      "    Min Reward over last 50 episodes: -16164.53\n",
      "    Max Reward over last 50 episodes: -16038.37\n",
      "    Evaluation Average Reward: -16025.06\n",
      "\n",
      "Running combination 179/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 1.0, 'gamma': 1.0, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "179it [44:53, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21885.23\n",
      "    Min Reward over last 50 episodes: -22034.98\n",
      "    Max Reward over last 50 episodes: -21735.48\n",
      "    Evaluation Average Reward: -21906.36\n",
      "\n",
      "Running combination 180/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.0, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180it [45:08, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20765.91\n",
      "    Min Reward over last 50 episodes: -20797.07\n",
      "    Max Reward over last 50 episodes: -20734.76\n",
      "    Evaluation Average Reward: -20687.18\n",
      "\n",
      "Running combination 181/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.0, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "181it [45:23, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23059.95\n",
      "    Min Reward over last 50 episodes: -23295.57\n",
      "    Max Reward over last 50 episodes: -22824.34\n",
      "    Evaluation Average Reward: -23125.39\n",
      "\n",
      "Running combination 182/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.6, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182it [45:38, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18204.75\n",
      "    Min Reward over last 50 episodes: -18330.19\n",
      "    Max Reward over last 50 episodes: -18079.30\n",
      "    Evaluation Average Reward: -18306.88\n",
      "\n",
      "Running combination 183/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 0.4, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "183it [45:53, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21743.00\n",
      "    Min Reward over last 50 episodes: -21890.14\n",
      "    Max Reward over last 50 episodes: -21595.86\n",
      "    Evaluation Average Reward: -21521.92\n",
      "\n",
      "Running combination 184/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.6, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "184it [46:08, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16038.23\n",
      "    Min Reward over last 50 episodes: -16090.63\n",
      "    Max Reward over last 50 episodes: -15985.84\n",
      "    Evaluation Average Reward: -16154.14\n",
      "\n",
      "Running combination 185/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 1.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "185it [46:23, 14.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16128.75\n",
      "    Min Reward over last 50 episodes: -16156.69\n",
      "    Max Reward over last 50 episodes: -16100.81\n",
      "    Evaluation Average Reward: -15953.53\n",
      "\n",
      "Running combination 186/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.6, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186it [46:38, 14.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26717.62\n",
      "    Min Reward over last 50 episodes: -26847.38\n",
      "    Max Reward over last 50 episodes: -26587.86\n",
      "    Evaluation Average Reward: -26904.10\n",
      "\n",
      "Running combination 187/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.2, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "187it [46:53, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19425.46\n",
      "    Min Reward over last 50 episodes: -19572.19\n",
      "    Max Reward over last 50 episodes: -19278.74\n",
      "    Evaluation Average Reward: -19418.19\n",
      "\n",
      "Running combination 188/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.9, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "188it [47:08, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29542.07\n",
      "    Min Reward over last 50 episodes: -29636.76\n",
      "    Max Reward over last 50 episodes: -29447.39\n",
      "    Evaluation Average Reward: -29661.78\n",
      "\n",
      "Running combination 189/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.9, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "189it [47:24, 15.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28106.31\n",
      "    Min Reward over last 50 episodes: -28221.06\n",
      "    Max Reward over last 50 episodes: -27991.56\n",
      "    Evaluation Average Reward: -28269.82\n",
      "\n",
      "Running combination 190/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.2, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "190it [47:39, 15.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19171.22\n",
      "    Min Reward over last 50 episodes: -19243.78\n",
      "    Max Reward over last 50 episodes: -19098.65\n",
      "    Evaluation Average Reward: -19121.64\n",
      "\n",
      "Running combination 191/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 0.2, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [47:53, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16033.77\n",
      "    Min Reward over last 50 episodes: -16086.33\n",
      "    Max Reward over last 50 episodes: -15981.21\n",
      "    Evaluation Average Reward: -16143.30\n",
      "\n",
      "Running combination 192/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.8, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192it [48:08, 14.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26791.94\n",
      "    Min Reward over last 50 episodes: -26800.88\n",
      "    Max Reward over last 50 episodes: -26783.01\n",
      "    Evaluation Average Reward: -27083.50\n",
      "\n",
      "Running combination 193/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 0.6, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "193it [48:23, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17972.29\n",
      "    Min Reward over last 50 episodes: -18020.39\n",
      "    Max Reward over last 50 episodes: -17924.20\n",
      "    Evaluation Average Reward: -18214.92\n",
      "\n",
      "Running combination 194/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 0.9, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "194it [48:38, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19486.24\n",
      "    Min Reward over last 50 episodes: -19525.22\n",
      "    Max Reward over last 50 episodes: -19447.25\n",
      "    Evaluation Average Reward: -19169.74\n",
      "\n",
      "Running combination 195/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.4, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "195it [48:52, 14.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19184.78\n",
      "    Min Reward over last 50 episodes: -19237.89\n",
      "    Max Reward over last 50 episodes: -19131.67\n",
      "    Evaluation Average Reward: -19278.04\n",
      "\n",
      "Running combination 196/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.8, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [49:07, 14.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20776.68\n",
      "    Min Reward over last 50 episodes: -20813.34\n",
      "    Max Reward over last 50 episodes: -20740.01\n",
      "    Evaluation Average Reward: -20676.22\n",
      "\n",
      "Running combination 197/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.0, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "197it [49:22, 14.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21673.20\n",
      "    Min Reward over last 50 episodes: -21828.86\n",
      "    Max Reward over last 50 episodes: -21517.53\n",
      "    Evaluation Average Reward: -21603.73\n",
      "\n",
      "Running combination 198/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 1.0, 'gamma': 0.9, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198it [49:37, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24624.96\n",
      "    Min Reward over last 50 episodes: -24737.41\n",
      "    Max Reward over last 50 episodes: -24512.52\n",
      "    Evaluation Average Reward: -24478.79\n",
      "\n",
      "Running combination 199/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.4, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [49:52, 14.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28314.92\n",
      "    Min Reward over last 50 episodes: -28350.72\n",
      "    Max Reward over last 50 episodes: -28279.13\n",
      "    Evaluation Average Reward: -28178.84\n",
      "\n",
      "Running combination 200/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.2, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [50:07, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28189.41\n",
      "    Min Reward over last 50 episodes: -28306.84\n",
      "    Max Reward over last 50 episodes: -28071.98\n",
      "    Evaluation Average Reward: -28417.16\n",
      "\n",
      "Running combination 201/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.4, 'gamma': 0.6, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [50:21, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18266.68\n",
      "    Min Reward over last 50 episodes: -18298.24\n",
      "    Max Reward over last 50 episodes: -18235.12\n",
      "    Evaluation Average Reward: -18104.85\n",
      "\n",
      "Running combination 202/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 1.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202it [50:37, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16088.21\n",
      "    Min Reward over last 50 episodes: -16138.15\n",
      "    Max Reward over last 50 episodes: -16038.27\n",
      "    Evaluation Average Reward: -16136.75\n",
      "\n",
      "Running combination 203/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.2, 'gamma': 0.8, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "203it [50:51, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21847.38\n",
      "    Min Reward over last 50 episodes: -22083.89\n",
      "    Max Reward over last 50 episodes: -21610.86\n",
      "    Evaluation Average Reward: -21539.31\n",
      "\n",
      "Running combination 204/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.6, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "204it [51:06, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17212.76\n",
      "    Min Reward over last 50 episodes: -17227.96\n",
      "    Max Reward over last 50 episodes: -17197.56\n",
      "    Evaluation Average Reward: -17131.82\n",
      "\n",
      "Running combination 205/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.4, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "205it [51:21, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19408.16\n",
      "    Min Reward over last 50 episodes: -19585.86\n",
      "    Max Reward over last 50 episodes: -19230.46\n",
      "    Evaluation Average Reward: -19470.11\n",
      "\n",
      "Running combination 206/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.2, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "206it [51:36, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29600.07\n",
      "    Min Reward over last 50 episodes: -29630.51\n",
      "    Max Reward over last 50 episodes: -29569.62\n",
      "    Evaluation Average Reward: -29442.96\n",
      "\n",
      "Running combination 207/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 1.0, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "207it [51:51, 14.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18348.97\n",
      "    Min Reward over last 50 episodes: -18354.49\n",
      "    Max Reward over last 50 episodes: -18343.45\n",
      "    Evaluation Average Reward: -18340.36\n",
      "\n",
      "Running combination 208/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.9, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "208it [52:06, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25523.80\n",
      "    Min Reward over last 50 episodes: -25793.63\n",
      "    Max Reward over last 50 episodes: -25253.97\n",
      "    Evaluation Average Reward: -25843.18\n",
      "\n",
      "Running combination 209/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.0, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209it [52:21, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16282.90\n",
      "    Min Reward over last 50 episodes: -16333.00\n",
      "    Max Reward over last 50 episodes: -16232.79\n",
      "    Evaluation Average Reward: -16332.31\n",
      "\n",
      "Running combination 210/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.4, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210it [52:37, 15.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19465.84\n",
      "    Min Reward over last 50 episodes: -19605.88\n",
      "    Max Reward over last 50 episodes: -19325.80\n",
      "    Evaluation Average Reward: -19005.15\n",
      "\n",
      "Running combination 211/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.0, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "211it [52:53, 15.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20748.64\n",
      "    Min Reward over last 50 episodes: -20814.63\n",
      "    Max Reward over last 50 episodes: -20682.66\n",
      "    Evaluation Average Reward: -20804.22\n",
      "\n",
      "Running combination 212/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 0.8, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "212it [53:09, 15.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24382.92\n",
      "    Min Reward over last 50 episodes: -24588.91\n",
      "    Max Reward over last 50 episodes: -24176.94\n",
      "    Evaluation Average Reward: -24456.13\n",
      "\n",
      "Running combination 213/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.2, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "213it [53:23, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17001.99\n",
      "    Min Reward over last 50 episodes: -17048.42\n",
      "    Max Reward over last 50 episodes: -16955.57\n",
      "    Evaluation Average Reward: -17304.72\n",
      "\n",
      "Running combination 214/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.9, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "214it [53:39, 15.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16038.30\n",
      "    Min Reward over last 50 episodes: -16066.55\n",
      "    Max Reward over last 50 episodes: -16010.04\n",
      "    Evaluation Average Reward: -16039.21\n",
      "\n",
      "Running combination 215/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.8, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "215it [53:54, 15.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16323.72\n",
      "    Min Reward over last 50 episodes: -16371.95\n",
      "    Max Reward over last 50 episodes: -16275.49\n",
      "    Evaluation Average Reward: -16243.38\n",
      "\n",
      "Running combination 216/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.2, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "216it [54:09, 15.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23246.51\n",
      "    Min Reward over last 50 episodes: -23296.64\n",
      "    Max Reward over last 50 episodes: -23196.37\n",
      "    Evaluation Average Reward: -23084.31\n",
      "\n",
      "Running combination 217/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.6, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "217it [54:23, 14.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20321.90\n",
      "    Min Reward over last 50 episodes: -20391.89\n",
      "    Max Reward over last 50 episodes: -20251.90\n",
      "    Evaluation Average Reward: -20412.13\n",
      "\n",
      "Running combination 218/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.4, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218it [54:38, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16063.94\n",
      "    Min Reward over last 50 episodes: -16076.58\n",
      "    Max Reward over last 50 episodes: -16051.29\n",
      "    Evaluation Average Reward: -16113.19\n",
      "\n",
      "Running combination 219/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 0.2, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "219it [54:53, 14.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23012.62\n",
      "    Min Reward over last 50 episodes: -23324.52\n",
      "    Max Reward over last 50 episodes: -22700.73\n",
      "    Evaluation Average Reward: -23031.07\n",
      "\n",
      "Running combination 220/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.4, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "220it [55:08, 14.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20533.99\n",
      "    Min Reward over last 50 episodes: -20567.32\n",
      "    Max Reward over last 50 episodes: -20500.67\n",
      "    Evaluation Average Reward: -20733.56\n",
      "\n",
      "Running combination 221/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.4, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [55:23, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19218.73\n",
      "    Min Reward over last 50 episodes: -19320.69\n",
      "    Max Reward over last 50 episodes: -19116.76\n",
      "    Evaluation Average Reward: -19560.09\n",
      "\n",
      "Running combination 222/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 1.0, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "222it [55:38, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17901.74\n",
      "    Min Reward over last 50 episodes: -17931.67\n",
      "    Max Reward over last 50 episodes: -17871.82\n",
      "    Evaluation Average Reward: -18098.94\n",
      "\n",
      "Running combination 223/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.8, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "223it [55:53, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18336.91\n",
      "    Min Reward over last 50 episodes: -18354.98\n",
      "    Max Reward over last 50 episodes: -18318.85\n",
      "    Evaluation Average Reward: -18301.54\n",
      "\n",
      "Running combination 224/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 0.2, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "224it [56:08, 14.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20635.26\n",
      "    Min Reward over last 50 episodes: -20715.84\n",
      "    Max Reward over last 50 episodes: -20554.68\n",
      "    Evaluation Average Reward: -20266.40\n",
      "\n",
      "Running combination 225/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 0.2, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [56:23, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18317.28\n",
      "    Min Reward over last 50 episodes: -18359.65\n",
      "    Max Reward over last 50 episodes: -18274.91\n",
      "    Evaluation Average Reward: -18048.62\n",
      "\n",
      "Running combination 226/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.9, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "226it [56:38, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24184.92\n",
      "    Min Reward over last 50 episodes: -24218.48\n",
      "    Max Reward over last 50 episodes: -24151.35\n",
      "    Evaluation Average Reward: -24358.49\n",
      "\n",
      "Running combination 227/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 1.0, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227it [56:53, 14.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20702.30\n",
      "    Min Reward over last 50 episodes: -20763.72\n",
      "    Max Reward over last 50 episodes: -20640.88\n",
      "    Evaluation Average Reward: -20644.00\n",
      "\n",
      "Running combination 228/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.0, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "228it [57:07, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16258.35\n",
      "    Min Reward over last 50 episodes: -16258.75\n",
      "    Max Reward over last 50 episodes: -16257.95\n",
      "    Evaluation Average Reward: -16387.26\n",
      "\n",
      "Running combination 229/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 0.8, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "229it [57:22, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25482.08\n",
      "    Min Reward over last 50 episodes: -25646.48\n",
      "    Max Reward over last 50 episodes: -25317.68\n",
      "    Evaluation Average Reward: -25687.93\n",
      "\n",
      "Running combination 230/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.0, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "230it [57:36, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21829.79\n",
      "    Min Reward over last 50 episodes: -22121.95\n",
      "    Max Reward over last 50 episodes: -21537.62\n",
      "    Evaluation Average Reward: -22119.87\n",
      "\n",
      "Running combination 231/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.8, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231it [57:51, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19245.11\n",
      "    Min Reward over last 50 episodes: -19493.44\n",
      "    Max Reward over last 50 episodes: -18996.78\n",
      "    Evaluation Average Reward: -19306.31\n",
      "\n",
      "Running combination 232/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.0, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "232it [58:06, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21837.37\n",
      "    Min Reward over last 50 episodes: -21983.70\n",
      "    Max Reward over last 50 episodes: -21691.04\n",
      "    Evaluation Average Reward: -21627.44\n",
      "\n",
      "Running combination 233/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.2, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "233it [58:21, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26812.83\n",
      "    Min Reward over last 50 episodes: -26924.86\n",
      "    Max Reward over last 50 episodes: -26700.80\n",
      "    Evaluation Average Reward: -26530.31\n",
      "\n",
      "Running combination 234/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.4, 'gamma': 0.8, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [58:35, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21926.99\n",
      "    Min Reward over last 50 episodes: -21982.63\n",
      "    Max Reward over last 50 episodes: -21871.35\n",
      "    Evaluation Average Reward: -21767.29\n",
      "\n",
      "Running combination 235/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.4, 'gamma': 0.6, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "235it [58:50, 14.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17129.53\n",
      "    Min Reward over last 50 episodes: -17178.17\n",
      "    Max Reward over last 50 episodes: -17080.89\n",
      "    Evaluation Average Reward: -17249.77\n",
      "\n",
      "Running combination 236/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.9, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "236it [59:05, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20437.64\n",
      "    Min Reward over last 50 episodes: -20526.68\n",
      "    Max Reward over last 50 episodes: -20348.60\n",
      "    Evaluation Average Reward: -20822.49\n",
      "\n",
      "Running combination 237/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.2, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "237it [59:19, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29450.15\n",
      "    Min Reward over last 50 episodes: -29577.53\n",
      "    Max Reward over last 50 episodes: -29322.77\n",
      "    Evaluation Average Reward: -29092.49\n",
      "\n",
      "Running combination 238/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.6, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "238it [59:34, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29191.69\n",
      "    Min Reward over last 50 episodes: -29196.43\n",
      "    Max Reward over last 50 episodes: -29186.96\n",
      "    Evaluation Average Reward: -29352.81\n",
      "\n",
      "Running combination 239/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.2, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "239it [59:48, 14.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19394.69\n",
      "    Min Reward over last 50 episodes: -19396.04\n",
      "    Max Reward over last 50 episodes: -19393.34\n",
      "    Evaluation Average Reward: -19284.13\n",
      "\n",
      "Running combination 240/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.8, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [1:00:03, 14.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16048.52\n",
      "    Min Reward over last 50 episodes: -16117.27\n",
      "    Max Reward over last 50 episodes: -15979.78\n",
      "    Evaluation Average Reward: -16084.45\n",
      "\n",
      "Running combination 241/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.4, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [1:00:18, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24324.29\n",
      "    Min Reward over last 50 episodes: -24504.01\n",
      "    Max Reward over last 50 episodes: -24144.57\n",
      "    Evaluation Average Reward: -24413.54\n",
      "\n",
      "Running combination 242/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.9, 'gamma': 0.4, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "242it [1:00:33, 14.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25358.38\n",
      "    Min Reward over last 50 episodes: -25448.72\n",
      "    Max Reward over last 50 episodes: -25268.04\n",
      "    Evaluation Average Reward: -25801.53\n",
      "\n",
      "Running combination 243/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.2, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "243it [1:00:47, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17908.03\n",
      "    Min Reward over last 50 episodes: -17950.19\n",
      "    Max Reward over last 50 episodes: -17865.86\n",
      "    Evaluation Average Reward: -18227.51\n",
      "\n",
      "Running combination 244/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.8, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [1:01:02, 14.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20561.37\n",
      "    Min Reward over last 50 episodes: -20829.92\n",
      "    Max Reward over last 50 episodes: -20292.83\n",
      "    Evaluation Average Reward: -20665.83\n",
      "\n",
      "Running combination 245/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.2, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [1:01:17, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28097.78\n",
      "    Min Reward over last 50 episodes: -28383.94\n",
      "    Max Reward over last 50 episodes: -27811.61\n",
      "    Evaluation Average Reward: -27923.31\n",
      "\n",
      "Running combination 246/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.9, 'gamma': 0.9, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "246it [1:01:32, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28250.99\n",
      "    Min Reward over last 50 episodes: -28291.28\n",
      "    Max Reward over last 50 episodes: -28210.70\n",
      "    Evaluation Average Reward: -27776.24\n",
      "\n",
      "Running combination 247/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.2, 'gamma': 0.8, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "247it [1:01:47, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -27049.77\n",
      "    Min Reward over last 50 episodes: -27135.19\n",
      "    Max Reward over last 50 episodes: -26964.34\n",
      "    Evaluation Average Reward: -27069.09\n",
      "\n",
      "Running combination 248/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.9, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "248it [1:02:02, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -27878.77\n",
      "    Min Reward over last 50 episodes: -27983.01\n",
      "    Max Reward over last 50 episodes: -27774.53\n",
      "    Evaluation Average Reward: -28342.85\n",
      "\n",
      "Running combination 249/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.8, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "249it [1:02:17, 14.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26706.07\n",
      "    Min Reward over last 50 episodes: -26900.44\n",
      "    Max Reward over last 50 episodes: -26511.69\n",
      "    Evaluation Average Reward: -26689.25\n",
      "\n",
      "Running combination 250/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 0.4, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [1:02:33, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26987.92\n",
      "    Min Reward over last 50 episodes: -27065.32\n",
      "    Max Reward over last 50 episodes: -26910.53\n",
      "    Evaluation Average Reward: -26597.98\n",
      "\n",
      "Running combination 251/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.2, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "251it [1:02:48, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29410.17\n",
      "    Min Reward over last 50 episodes: -29539.40\n",
      "    Max Reward over last 50 episodes: -29280.93\n",
      "    Evaluation Average Reward: -29470.72\n",
      "\n",
      "Running combination 252/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.9, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "252it [1:03:04, 15.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29415.72\n",
      "    Min Reward over last 50 episodes: -29623.36\n",
      "    Max Reward over last 50 episodes: -29208.07\n",
      "    Evaluation Average Reward: -29082.72\n",
      "\n",
      "Running combination 253/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.9, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "253it [1:03:19, 15.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24376.54\n",
      "    Min Reward over last 50 episodes: -24611.57\n",
      "    Max Reward over last 50 episodes: -24141.51\n",
      "    Evaluation Average Reward: -24294.06\n",
      "\n",
      "Running combination 254/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.4, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "254it [1:03:35, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20441.48\n",
      "    Min Reward over last 50 episodes: -20626.79\n",
      "    Max Reward over last 50 episodes: -20256.17\n",
      "    Evaluation Average Reward: -20640.56\n",
      "\n",
      "Running combination 255/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.9, 'gamma': 0.6, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "255it [1:03:51, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18238.66\n",
      "    Min Reward over last 50 episodes: -18290.48\n",
      "    Max Reward over last 50 episodes: -18186.84\n",
      "    Evaluation Average Reward: -18158.13\n",
      "\n",
      "Running combination 256/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.2, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [1:04:06, 15.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17973.34\n",
      "    Min Reward over last 50 episodes: -18053.35\n",
      "    Max Reward over last 50 episodes: -17893.32\n",
      "    Evaluation Average Reward: -18414.84\n",
      "\n",
      "Running combination 257/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.9, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [1:04:22, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23052.01\n",
      "    Min Reward over last 50 episodes: -23286.61\n",
      "    Max Reward over last 50 episodes: -22817.41\n",
      "    Evaluation Average Reward: -23378.18\n",
      "\n",
      "Running combination 258/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.4, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "258it [1:04:37, 15.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25559.41\n",
      "    Min Reward over last 50 episodes: -25726.16\n",
      "    Max Reward over last 50 episodes: -25392.66\n",
      "    Evaluation Average Reward: -25488.05\n",
      "\n",
      "Running combination 259/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 1.0, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "259it [1:04:52, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21718.44\n",
      "    Min Reward over last 50 episodes: -21869.37\n",
      "    Max Reward over last 50 episodes: -21567.50\n",
      "    Evaluation Average Reward: -21534.43\n",
      "\n",
      "Running combination 260/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.8, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "260it [1:05:07, 15.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21779.23\n",
      "    Min Reward over last 50 episodes: -21919.01\n",
      "    Max Reward over last 50 episodes: -21639.45\n",
      "    Evaluation Average Reward: -21828.73\n",
      "\n",
      "Running combination 261/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 0.4, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "261it [1:05:22, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16893.62\n",
      "    Min Reward over last 50 episodes: -16955.59\n",
      "    Max Reward over last 50 episodes: -16831.64\n",
      "    Evaluation Average Reward: -17136.24\n",
      "SKIPPING\n",
      "\n",
      "Running combination 262/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.6, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "262it [1:05:37, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -22939.25\n",
      "    Min Reward over last 50 episodes: -22942.02\n",
      "    Max Reward over last 50 episodes: -22936.48\n",
      "    Evaluation Average Reward: -23393.26\n",
      "\n",
      "Running combination 263/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.8, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "263it [1:05:53, 15.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16320.19\n",
      "    Min Reward over last 50 episodes: -16349.62\n",
      "    Max Reward over last 50 episodes: -16290.76\n",
      "    Evaluation Average Reward: -16192.04\n",
      "\n",
      "Running combination 264/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.2, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "264it [1:06:08, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25801.17\n",
      "    Min Reward over last 50 episodes: -25866.76\n",
      "    Max Reward over last 50 episodes: -25735.58\n",
      "    Evaluation Average Reward: -25312.66\n",
      "\n",
      "Running combination 265/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.2, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "265it [1:06:24, 15.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28137.45\n",
      "    Min Reward over last 50 episodes: -28325.83\n",
      "    Max Reward over last 50 episodes: -27949.07\n",
      "    Evaluation Average Reward: -28250.13\n",
      "\n",
      "Running combination 266/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.8, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "266it [1:06:39, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24137.66\n",
      "    Min Reward over last 50 episodes: -24146.14\n",
      "    Max Reward over last 50 episodes: -24129.18\n",
      "    Evaluation Average Reward: -24564.16\n",
      "\n",
      "Running combination 267/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.8, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "267it [1:06:54, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28016.07\n",
      "    Min Reward over last 50 episodes: -28205.29\n",
      "    Max Reward over last 50 episodes: -27826.84\n",
      "    Evaluation Average Reward: -28481.46\n",
      "\n",
      "Running combination 268/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.0, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "268it [1:07:09, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21582.53\n",
      "    Min Reward over last 50 episodes: -21604.36\n",
      "    Max Reward over last 50 episodes: -21560.70\n",
      "    Evaluation Average Reward: -21819.96\n",
      "\n",
      "Running combination 269/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 1.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "269it [1:07:24, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16074.67\n",
      "    Min Reward over last 50 episodes: -16127.49\n",
      "    Max Reward over last 50 episodes: -16021.84\n",
      "    Evaluation Average Reward: -16125.04\n",
      "\n",
      "Running combination 270/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.2, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "270it [1:07:39, 15.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26728.86\n",
      "    Min Reward over last 50 episodes: -26776.13\n",
      "    Max Reward over last 50 episodes: -26681.58\n",
      "    Evaluation Average Reward: -26614.76\n",
      "\n",
      "Running combination 271/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.6, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [1:07:53, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16071.31\n",
      "    Min Reward over last 50 episodes: -16076.60\n",
      "    Max Reward over last 50 episodes: -16066.03\n",
      "    Evaluation Average Reward: -16016.91\n",
      "\n",
      "Running combination 272/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.6, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "272it [1:08:08, 14.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17023.67\n",
      "    Min Reward over last 50 episodes: -17027.60\n",
      "    Max Reward over last 50 episodes: -17019.73\n",
      "    Evaluation Average Reward: -17041.99\n",
      "\n",
      "Running combination 273/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.9, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273it [1:08:24, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16057.59\n",
      "    Min Reward over last 50 episodes: -16127.29\n",
      "    Max Reward over last 50 episodes: -15987.89\n",
      "    Evaluation Average Reward: -16006.36\n",
      "\n",
      "Running combination 274/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.6, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [1:08:39, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -22908.95\n",
      "    Min Reward over last 50 episodes: -23070.13\n",
      "    Max Reward over last 50 episodes: -22747.77\n",
      "    Evaluation Average Reward: -23335.22\n",
      "\n",
      "Running combination 275/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.9, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "275it [1:08:54, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20437.65\n",
      "    Min Reward over last 50 episodes: -20497.09\n",
      "    Max Reward over last 50 episodes: -20378.22\n",
      "    Evaluation Average Reward: -20711.08\n",
      "\n",
      "Running combination 276/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 1.0, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "276it [1:09:10, 15.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29363.78\n",
      "    Min Reward over last 50 episodes: -29615.79\n",
      "    Max Reward over last 50 episodes: -29111.77\n",
      "    Evaluation Average Reward: -29609.41\n",
      "\n",
      "Running combination 277/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.4, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "277it [1:09:25, 15.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16328.74\n",
      "    Min Reward over last 50 episodes: -16383.21\n",
      "    Max Reward over last 50 episodes: -16274.26\n",
      "    Evaluation Average Reward: -16324.53\n",
      "\n",
      "Running combination 278/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 1.0, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "278it [1:09:41, 15.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26714.51\n",
      "    Min Reward over last 50 episodes: -26893.40\n",
      "    Max Reward over last 50 episodes: -26535.63\n",
      "    Evaluation Average Reward: -27083.90\n",
      "\n",
      "Running combination 279/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.2, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "279it [1:09:56, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16245.49\n",
      "    Min Reward over last 50 episodes: -16296.18\n",
      "    Max Reward over last 50 episodes: -16194.81\n",
      "    Evaluation Average Reward: -16312.97\n",
      "\n",
      "Running combination 280/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 0.4, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [1:10:15, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17176.11\n",
      "    Min Reward over last 50 episodes: -17190.44\n",
      "    Max Reward over last 50 episodes: -17161.77\n",
      "    Evaluation Average Reward: -17134.48\n",
      "\n",
      "Running combination 281/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.6, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "281it [1:10:36, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28049.91\n",
      "    Min Reward over last 50 episodes: -28270.27\n",
      "    Max Reward over last 50 episodes: -27829.55\n",
      "    Evaluation Average Reward: -27804.42\n",
      "\n",
      "Running combination 282/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.2, 'gamma': 0.8, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "282it [1:10:52, 17.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28194.12\n",
      "    Min Reward over last 50 episodes: -28263.14\n",
      "    Max Reward over last 50 episodes: -28125.10\n",
      "    Evaluation Average Reward: -28075.11\n",
      "\n",
      "Running combination 283/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "283it [1:11:09, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16117.14\n",
      "    Min Reward over last 50 episodes: -16159.57\n",
      "    Max Reward over last 50 episodes: -16074.70\n",
      "    Evaluation Average Reward: -16092.53\n",
      "\n",
      "Running combination 284/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.2, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "284it [1:11:26, 17.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28043.30\n",
      "    Min Reward over last 50 episodes: -28233.28\n",
      "    Max Reward over last 50 episodes: -27853.32\n",
      "    Evaluation Average Reward: -28111.24\n",
      "\n",
      "Running combination 285/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 0.0, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "285it [1:11:43, 17.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24523.42\n",
      "    Min Reward over last 50 episodes: -24648.46\n",
      "    Max Reward over last 50 episodes: -24398.38\n",
      "    Evaluation Average Reward: -24492.71\n",
      "\n",
      "Running combination 286/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 1.0, 'gamma': 0.6, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "286it [1:12:00, 17.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25721.65\n",
      "    Min Reward over last 50 episodes: -25753.02\n",
      "    Max Reward over last 50 episodes: -25690.28\n",
      "    Evaluation Average Reward: -25370.04\n",
      "\n",
      "Running combination 287/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.9, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "287it [1:12:17, 17.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19513.49\n",
      "    Min Reward over last 50 episodes: -19668.15\n",
      "    Max Reward over last 50 episodes: -19358.83\n",
      "    Evaluation Average Reward: -19537.26\n",
      "\n",
      "Running combination 288/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.6, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "288it [1:12:33, 16.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16986.28\n",
      "    Min Reward over last 50 episodes: -17079.22\n",
      "    Max Reward over last 50 episodes: -16893.34\n",
      "    Evaluation Average Reward: -16960.82\n",
      "\n",
      "Running combination 289/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 0.2, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "289it [1:12:50, 16.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19568.74\n",
      "    Min Reward over last 50 episodes: -19583.71\n",
      "    Max Reward over last 50 episodes: -19553.76\n",
      "    Evaluation Average Reward: -19173.64\n",
      "\n",
      "Running combination 290/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.9, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "290it [1:13:07, 16.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16025.43\n",
      "    Min Reward over last 50 episodes: -16025.98\n",
      "    Max Reward over last 50 episodes: -16024.89\n",
      "    Evaluation Average Reward: -16052.62\n",
      "\n",
      "Running combination 291/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.6, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "291it [1:13:24, 16.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25805.84\n",
      "    Min Reward over last 50 episodes: -25815.41\n",
      "    Max Reward over last 50 episodes: -25796.26\n",
      "    Evaluation Average Reward: -25796.08\n",
      "\n",
      "Running combination 292/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.9, 'gamma': 0.8, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "292it [1:13:40, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18147.99\n",
      "    Min Reward over last 50 episodes: -18244.53\n",
      "    Max Reward over last 50 episodes: -18051.46\n",
      "    Evaluation Average Reward: -18379.31\n",
      "\n",
      "Running combination 293/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.0, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "293it [1:13:56, 16.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -27921.58\n",
      "    Min Reward over last 50 episodes: -27950.87\n",
      "    Max Reward over last 50 episodes: -27892.28\n",
      "    Evaluation Average Reward: -27833.31\n",
      "\n",
      "Running combination 294/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.9, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "294it [1:14:14, 16.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28262.85\n",
      "    Min Reward over last 50 episodes: -28294.76\n",
      "    Max Reward over last 50 episodes: -28230.94\n",
      "    Evaluation Average Reward: -28403.32\n",
      "\n",
      "Running combination 295/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.6, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "295it [1:14:30, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20623.82\n",
      "    Min Reward over last 50 episodes: -20737.66\n",
      "    Max Reward over last 50 episodes: -20509.98\n",
      "    Evaluation Average Reward: -20871.10\n",
      "\n",
      "Running combination 296/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.6, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "296it [1:14:46, 16.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20270.72\n",
      "    Min Reward over last 50 episodes: -20323.78\n",
      "    Max Reward over last 50 episodes: -20217.66\n",
      "    Evaluation Average Reward: -20337.40\n",
      "\n",
      "Running combination 297/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.2, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "297it [1:15:03, 16.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26756.14\n",
      "    Min Reward over last 50 episodes: -26920.22\n",
      "    Max Reward over last 50 episodes: -26592.05\n",
      "    Evaluation Average Reward: -26776.74\n",
      "\n",
      "Running combination 298/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.9, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "298it [1:15:20, 16.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25568.66\n",
      "    Min Reward over last 50 episodes: -25830.92\n",
      "    Max Reward over last 50 episodes: -25306.40\n",
      "    Evaluation Average Reward: -25924.71\n",
      "\n",
      "Running combination 299/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.9, 'gamma': 0.9, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "299it [1:15:37, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16169.40\n",
      "    Min Reward over last 50 episodes: -16193.78\n",
      "    Max Reward over last 50 episodes: -16145.02\n",
      "    Evaluation Average Reward: -16282.34\n",
      "\n",
      "Running combination 300/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.8, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [1:15:53, 16.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -27871.59\n",
      "    Min Reward over last 50 episodes: -28010.85\n",
      "    Max Reward over last 50 episodes: -27732.33\n",
      "    Evaluation Average Reward: -28386.07\n",
      "\n",
      "Running combination 301/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 0.0, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [1:16:09, 16.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26598.79\n",
      "    Min Reward over last 50 episodes: -26679.26\n",
      "    Max Reward over last 50 episodes: -26518.32\n",
      "    Evaluation Average Reward: -26770.92\n",
      "\n",
      "Running combination 302/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.8, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "302it [1:16:26, 16.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29163.29\n",
      "    Min Reward over last 50 episodes: -29177.51\n",
      "    Max Reward over last 50 episodes: -29149.06\n",
      "    Evaluation Average Reward: -29659.95\n",
      "\n",
      "Running combination 303/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.9, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "303it [1:16:43, 16.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24336.42\n",
      "    Min Reward over last 50 episodes: -24393.40\n",
      "    Max Reward over last 50 episodes: -24279.44\n",
      "    Evaluation Average Reward: -24147.51\n",
      "\n",
      "Running combination 304/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.2, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "304it [1:16:59, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21828.91\n",
      "    Min Reward over last 50 episodes: -21922.63\n",
      "    Max Reward over last 50 episodes: -21735.19\n",
      "    Evaluation Average Reward: -21776.85\n",
      "\n",
      "Running combination 305/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.9, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "305it [1:17:16, 16.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25631.06\n",
      "    Min Reward over last 50 episodes: -25670.04\n",
      "    Max Reward over last 50 episodes: -25592.07\n",
      "    Evaluation Average Reward: -25609.98\n",
      "\n",
      "Running combination 306/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.4, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "306it [1:17:31, 16.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28105.60\n",
      "    Min Reward over last 50 episodes: -28178.83\n",
      "    Max Reward over last 50 episodes: -28032.37\n",
      "    Evaluation Average Reward: -27967.21\n",
      "\n",
      "Running combination 307/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.0, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "307it [1:17:48, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29347.69\n",
      "    Min Reward over last 50 episodes: -29633.91\n",
      "    Max Reward over last 50 episodes: -29061.48\n",
      "    Evaluation Average Reward: -29626.01\n",
      "\n",
      "Running combination 308/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.9, 'gamma': 0.2, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "308it [1:18:05, 16.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17197.82\n",
      "    Min Reward over last 50 episodes: -17203.37\n",
      "    Max Reward over last 50 episodes: -17192.28\n",
      "    Evaluation Average Reward: -17034.59\n",
      "\n",
      "Running combination 309/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.2, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "309it [1:18:21, 16.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25405.28\n",
      "    Min Reward over last 50 episodes: -25454.99\n",
      "    Max Reward over last 50 episodes: -25355.58\n",
      "    Evaluation Average Reward: -25330.57\n",
      "\n",
      "Running combination 310/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.8, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "310it [1:18:37, 16.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29165.69\n",
      "    Min Reward over last 50 episodes: -29308.65\n",
      "    Max Reward over last 50 episodes: -29022.73\n",
      "    Evaluation Average Reward: -29051.40\n",
      "\n",
      "Running combination 311/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.4, 'gamma': 0.9, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "311it [1:18:53, 16.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20553.23\n",
      "    Min Reward over last 50 episodes: -20709.68\n",
      "    Max Reward over last 50 episodes: -20396.78\n",
      "    Evaluation Average Reward: -20407.27\n",
      "\n",
      "Running combination 312/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.2, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "312it [1:19:10, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19415.74\n",
      "    Min Reward over last 50 episodes: -19446.28\n",
      "    Max Reward over last 50 episodes: -19385.21\n",
      "    Evaluation Average Reward: -18988.07\n",
      "\n",
      "Running combination 313/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.9, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [1:19:26, 16.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21549.42\n",
      "    Min Reward over last 50 episodes: -21570.51\n",
      "    Max Reward over last 50 episodes: -21528.33\n",
      "    Evaluation Average Reward: -21511.51\n",
      "\n",
      "Running combination 314/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 0.0, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "314it [1:19:42, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25721.96\n",
      "    Min Reward over last 50 episodes: -25825.11\n",
      "    Max Reward over last 50 episodes: -25618.81\n",
      "    Evaluation Average Reward: -25262.60\n",
      "\n",
      "Running combination 315/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.2, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "315it [1:19:59, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -27807.76\n",
      "    Min Reward over last 50 episodes: -27820.89\n",
      "    Max Reward over last 50 episodes: -27794.63\n",
      "    Evaluation Average Reward: -28434.37\n",
      "\n",
      "Running combination 316/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.2, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "316it [1:20:16, 16.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24039.28\n",
      "    Min Reward over last 50 episodes: -24134.20\n",
      "    Max Reward over last 50 episodes: -23944.36\n",
      "    Evaluation Average Reward: -24632.57\n",
      "\n",
      "Running combination 317/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.9, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317it [1:20:32, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17044.64\n",
      "    Min Reward over last 50 episodes: -17097.03\n",
      "    Max Reward over last 50 episodes: -16992.25\n",
      "    Evaluation Average Reward: -17008.37\n",
      "\n",
      "Running combination 318/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.8, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "318it [1:20:48, 16.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20639.92\n",
      "    Min Reward over last 50 episodes: -20709.05\n",
      "    Max Reward over last 50 episodes: -20570.78\n",
      "    Evaluation Average Reward: -20292.82\n",
      "\n",
      "Running combination 319/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.9, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "319it [1:21:05, 16.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21651.19\n",
      "    Min Reward over last 50 episodes: -21766.76\n",
      "    Max Reward over last 50 episodes: -21535.63\n",
      "    Evaluation Average Reward: -21969.31\n",
      "\n",
      "Running combination 320/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.8, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "320it [1:21:21, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16093.93\n",
      "    Min Reward over last 50 episodes: -16110.53\n",
      "    Max Reward over last 50 episodes: -16077.33\n",
      "    Evaluation Average Reward: -16150.93\n",
      "\n",
      "Running combination 321/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.0, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "321it [1:21:39, 16.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20701.73\n",
      "    Min Reward over last 50 episodes: -20767.48\n",
      "    Max Reward over last 50 episodes: -20635.97\n",
      "    Evaluation Average Reward: -20826.07\n",
      "\n",
      "Running combination 322/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.8, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "322it [1:21:57, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23292.79\n",
      "    Min Reward over last 50 episodes: -23324.61\n",
      "    Max Reward over last 50 episodes: -23260.96\n",
      "    Evaluation Average Reward: -22865.12\n",
      "\n",
      "Running combination 323/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.6, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "323it [1:22:14, 17.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17114.82\n",
      "    Min Reward over last 50 episodes: -17159.80\n",
      "    Max Reward over last 50 episodes: -17069.83\n",
      "    Evaluation Average Reward: -16944.76\n",
      "\n",
      "Running combination 324/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.6, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "324it [1:22:31, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25814.36\n",
      "    Min Reward over last 50 episodes: -25855.83\n",
      "    Max Reward over last 50 episodes: -25772.88\n",
      "    Evaluation Average Reward: -25341.67\n",
      "\n",
      "Running combination 325/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.4, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "325it [1:22:48, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17120.67\n",
      "    Min Reward over last 50 episodes: -17131.26\n",
      "    Max Reward over last 50 episodes: -17110.09\n",
      "    Evaluation Average Reward: -17241.20\n",
      "\n",
      "Running combination 326/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.8, 'gamma': 0.8, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "326it [1:23:05, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19094.30\n",
      "    Min Reward over last 50 episodes: -19134.00\n",
      "    Max Reward over last 50 episodes: -19054.61\n",
      "    Evaluation Average Reward: -19091.05\n",
      "SKIPPING\n",
      "\n",
      "Running combination 327/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.2, 'gamma': 0.6, 'no_change_after_lap': 130}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "327it [1:23:21, 16.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -21868.79\n",
      "    Min Reward over last 50 episodes: -21961.27\n",
      "    Max Reward over last 50 episodes: -21776.31\n",
      "    Evaluation Average Reward: -21669.12\n",
      "\n",
      "Running combination 328/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.2, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "328it [1:23:37, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16058.78\n",
      "    Min Reward over last 50 episodes: -16082.39\n",
      "    Max Reward over last 50 episodes: -16035.17\n",
      "    Evaluation Average Reward: -16083.33\n",
      "\n",
      "Running combination 329/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 0.9, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "329it [1:23:54, 16.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19418.42\n",
      "    Min Reward over last 50 episodes: -19542.25\n",
      "    Max Reward over last 50 episodes: -19294.60\n",
      "    Evaluation Average Reward: -19009.16\n",
      "\n",
      "Running combination 330/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 0.4, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "330it [1:24:11, 16.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28289.46\n",
      "    Min Reward over last 50 episodes: -28391.59\n",
      "    Max Reward over last 50 episodes: -28187.33\n",
      "    Evaluation Average Reward: -28177.56\n",
      "\n",
      "Running combination 331/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.4, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "331it [1:24:27, 16.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25529.26\n",
      "    Min Reward over last 50 episodes: -25581.31\n",
      "    Max Reward over last 50 episodes: -25477.21\n",
      "    Evaluation Average Reward: -25778.46\n",
      "\n",
      "Running combination 332/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.9, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "332it [1:24:44, 16.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19404.08\n",
      "    Min Reward over last 50 episodes: -19478.11\n",
      "    Max Reward over last 50 episodes: -19330.06\n",
      "    Evaluation Average Reward: -19322.76\n",
      "\n",
      "Running combination 333/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 1.0, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "333it [1:25:00, 16.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16288.20\n",
      "    Min Reward over last 50 episodes: -16363.24\n",
      "    Max Reward over last 50 episodes: -16213.17\n",
      "    Evaluation Average Reward: -16437.38\n",
      "\n",
      "Running combination 334/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.9, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "334it [1:25:17, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26863.85\n",
      "    Min Reward over last 50 episodes: -26997.96\n",
      "    Max Reward over last 50 episodes: -26729.73\n",
      "    Evaluation Average Reward: -27038.00\n",
      "\n",
      "Running combination 335/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 1.0, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "335it [1:25:33, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25678.32\n",
      "    Min Reward over last 50 episodes: -25679.75\n",
      "    Max Reward over last 50 episodes: -25676.90\n",
      "    Evaluation Average Reward: -25345.28\n",
      "\n",
      "Running combination 336/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 1.0, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "336it [1:25:50, 16.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18207.25\n",
      "    Min Reward over last 50 episodes: -18358.75\n",
      "    Max Reward over last 50 episodes: -18055.76\n",
      "    Evaluation Average Reward: -18442.04\n",
      "\n",
      "Running combination 337/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.2, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "337it [1:26:06, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23207.71\n",
      "    Min Reward over last 50 episodes: -23310.74\n",
      "    Max Reward over last 50 episodes: -23104.67\n",
      "    Evaluation Average Reward: -22938.67\n",
      "\n",
      "Running combination 338/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 1.0, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [1:26:22, 16.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19263.28\n",
      "    Min Reward over last 50 episodes: -19531.02\n",
      "    Max Reward over last 50 episodes: -18995.55\n",
      "    Evaluation Average Reward: -19557.03\n",
      "\n",
      "Running combination 339/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.4, 'gamma': 0.4, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "339it [1:26:39, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -15989.96\n",
      "    Min Reward over last 50 episodes: -15990.07\n",
      "    Max Reward over last 50 episodes: -15989.85\n",
      "    Evaluation Average Reward: -16061.13\n",
      "\n",
      "Running combination 340/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 0.6, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "340it [1:26:55, 16.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29582.23\n",
      "    Min Reward over last 50 episodes: -29621.41\n",
      "    Max Reward over last 50 episodes: -29543.05\n",
      "    Evaluation Average Reward: -29465.97\n",
      "\n",
      "Running combination 341/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.6, 'gamma': 0.0, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "341it [1:27:11, 16.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24130.03\n",
      "    Min Reward over last 50 episodes: -24249.40\n",
      "    Max Reward over last 50 episodes: -24010.65\n",
      "    Evaluation Average Reward: -24391.88\n",
      "\n",
      "Running combination 342/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.4, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "342it [1:27:27, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -27016.76\n",
      "    Min Reward over last 50 episodes: -27052.99\n",
      "    Max Reward over last 50 episodes: -26980.53\n",
      "    Evaluation Average Reward: -26888.50\n",
      "\n",
      "Running combination 343/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.6, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343it [1:27:44, 16.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16232.32\n",
      "    Min Reward over last 50 episodes: -16265.37\n",
      "    Max Reward over last 50 episodes: -16199.26\n",
      "    Evaluation Average Reward: -16322.03\n",
      "\n",
      "Running combination 344/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.9, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "344it [1:28:01, 16.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29288.39\n",
      "    Min Reward over last 50 episodes: -29463.19\n",
      "    Max Reward over last 50 episodes: -29113.58\n",
      "    Evaluation Average Reward: -29435.60\n",
      "\n",
      "Running combination 345/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.4, 'gamma': 0.0, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "345it [1:28:16, 16.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24545.40\n",
      "    Min Reward over last 50 episodes: -24599.17\n",
      "    Max Reward over last 50 episodes: -24491.63\n",
      "    Evaluation Average Reward: -24670.36\n",
      "\n",
      "Running combination 346/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.4, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "346it [1:28:33, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16029.25\n",
      "    Min Reward over last 50 episodes: -16058.82\n",
      "    Max Reward over last 50 episodes: -15999.67\n",
      "    Evaluation Average Reward: -16133.13\n",
      "\n",
      "Running combination 347/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.8, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "347it [1:28:49, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20575.12\n",
      "    Min Reward over last 50 episodes: -20850.35\n",
      "    Max Reward over last 50 episodes: -20299.90\n",
      "    Evaluation Average Reward: -20823.06\n",
      "\n",
      "Running combination 348/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.2, 'gamma': 0.0, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "348it [1:29:06, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24369.38\n",
      "    Min Reward over last 50 episodes: -24491.29\n",
      "    Max Reward over last 50 episodes: -24247.47\n",
      "    Evaluation Average Reward: -24528.25\n",
      "\n",
      "Running combination 349/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.999, 'alpha': 0.6, 'gamma': 1.0, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "349it [1:29:22, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20476.60\n",
      "    Min Reward over last 50 episodes: -20760.46\n",
      "    Max Reward over last 50 episodes: -20192.75\n",
      "    Evaluation Average Reward: -20614.08\n",
      "\n",
      "Running combination 350/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "350it [1:29:38, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16119.28\n",
      "    Min Reward over last 50 episodes: -16137.51\n",
      "    Max Reward over last 50 episodes: -16101.04\n",
      "    Evaluation Average Reward: -15968.54\n",
      "\n",
      "Running combination 351/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.2, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "351it [1:29:55, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24461.29\n",
      "    Min Reward over last 50 episodes: -24521.82\n",
      "    Max Reward over last 50 episodes: -24400.76\n",
      "    Evaluation Average Reward: -24141.21\n",
      "\n",
      "Running combination 352/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.9, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "352it [1:30:12, 16.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19301.06\n",
      "    Min Reward over last 50 episodes: -19605.21\n",
      "    Max Reward over last 50 episodes: -18996.92\n",
      "    Evaluation Average Reward: -19554.30\n",
      "\n",
      "Running combination 353/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.9, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "353it [1:30:29, 16.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -19312.13\n",
      "    Min Reward over last 50 episodes: -19314.02\n",
      "    Max Reward over last 50 episodes: -19310.24\n",
      "    Evaluation Average Reward: -19114.70\n",
      "\n",
      "Running combination 354/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.4, 'gamma': 1.0, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "354it [1:30:45, 16.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17120.60\n",
      "    Min Reward over last 50 episodes: -17160.69\n",
      "    Max Reward over last 50 episodes: -17080.52\n",
      "    Evaluation Average Reward: -17094.54\n",
      "\n",
      "Running combination 355/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.4, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "355it [1:31:01, 16.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16033.58\n",
      "    Min Reward over last 50 episodes: -16063.35\n",
      "    Max Reward over last 50 episodes: -16003.82\n",
      "    Evaluation Average Reward: -16033.78\n",
      "\n",
      "Running combination 356/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "356it [1:31:17, 16.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16067.96\n",
      "    Min Reward over last 50 episodes: -16145.61\n",
      "    Max Reward over last 50 episodes: -15990.31\n",
      "    Evaluation Average Reward: -16124.63\n",
      "\n",
      "Running combination 357/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 1.0, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "357it [1:31:34, 16.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23107.29\n",
      "    Min Reward over last 50 episodes: -23227.01\n",
      "    Max Reward over last 50 episodes: -22987.57\n",
      "    Evaluation Average Reward: -23257.80\n",
      "\n",
      "Running combination 358/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.2, 'gamma': 0.0, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "358it [1:31:52, 16.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26904.29\n",
      "    Min Reward over last 50 episodes: -26963.63\n",
      "    Max Reward over last 50 episodes: -26844.96\n",
      "    Evaluation Average Reward: -27032.27\n",
      "\n",
      "Running combination 359/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.9, 'gamma': 0.6, 'no_change_after_lap': 110}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [1:32:09, 16.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -26989.71\n",
      "    Min Reward over last 50 episodes: -26990.55\n",
      "    Max Reward over last 50 episodes: -26988.87\n",
      "    Evaluation Average Reward: -27011.78\n",
      "\n",
      "Running combination 360/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.99, 'alpha': 0.2, 'gamma': 1.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "360it [1:32:27, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16104.19\n",
      "    Min Reward over last 50 episodes: -16136.21\n",
      "    Max Reward over last 50 episodes: -16072.16\n",
      "    Evaluation Average Reward: -15988.88\n",
      "\n",
      "Running combination 361/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.0, 'no_change_after_lap': 105}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "361it [1:32:44, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -28281.24\n",
      "    Min Reward over last 50 episodes: -28325.75\n",
      "    Max Reward over last 50 episodes: -28236.73\n",
      "    Evaluation Average Reward: -28050.82\n",
      "\n",
      "Running combination 362/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.99, 'alpha': 0.2, 'gamma': 1.0, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "362it [1:33:03, 17.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25567.74\n",
      "    Min Reward over last 50 episodes: -25926.20\n",
      "    Max Reward over last 50 episodes: -25209.29\n",
      "    Evaluation Average Reward: -25265.89\n",
      "\n",
      "Running combination 363/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.9, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "363it [1:33:22, 18.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16898.90\n",
      "    Min Reward over last 50 episodes: -16943.77\n",
      "    Max Reward over last 50 episodes: -16854.03\n",
      "    Evaluation Average Reward: -17033.51\n",
      "\n",
      "Running combination 364/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 0.8, 'gamma': 1.0, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "364it [1:33:42, 18.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29303.91\n",
      "    Min Reward over last 50 episodes: -29371.57\n",
      "    Max Reward over last 50 episodes: -29236.24\n",
      "    Evaluation Average Reward: -29366.36\n",
      "\n",
      "Running combination 365/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 0.8, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "365it [1:34:01, 18.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16028.44\n",
      "    Min Reward over last 50 episodes: -16041.71\n",
      "    Max Reward over last 50 episodes: -16015.16\n",
      "    Evaluation Average Reward: -16033.37\n",
      "\n",
      "Running combination 366/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 0.9, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "366it [1:34:20, 18.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23207.38\n",
      "    Min Reward over last 50 episodes: -23279.93\n",
      "    Max Reward over last 50 episodes: -23134.82\n",
      "    Evaluation Average Reward: -23259.31\n",
      "\n",
      "Running combination 367/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.4, 'gamma': 0.8, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "367it [1:34:37, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20492.17\n",
      "    Min Reward over last 50 episodes: -20663.37\n",
      "    Max Reward over last 50 episodes: -20320.96\n",
      "    Evaluation Average Reward: -20498.42\n",
      "\n",
      "Running combination 368/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 0.0, 'no_change_after_lap': 160}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "368it [1:34:54, 17.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16109.72\n",
      "    Min Reward over last 50 episodes: -16146.10\n",
      "    Max Reward over last 50 episodes: -16073.34\n",
      "    Evaluation Average Reward: -16115.19\n",
      "\n",
      "Running combination 369/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.8, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "369it [1:35:13, 18.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24212.03\n",
      "    Min Reward over last 50 episodes: -24390.61\n",
      "    Max Reward over last 50 episodes: -24033.45\n",
      "    Evaluation Average Reward: -24068.74\n",
      "\n",
      "Running combination 370/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.0, 'gamma': 0.0, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "370it [1:35:30, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25496.07\n",
      "    Min Reward over last 50 episodes: -25794.43\n",
      "    Max Reward over last 50 episodes: -25197.72\n",
      "    Evaluation Average Reward: -25592.65\n",
      "\n",
      "Running combination 371/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 0.6, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "371it [1:35:46, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24002.34\n",
      "    Min Reward over last 50 episodes: -24027.66\n",
      "    Max Reward over last 50 episodes: -23977.01\n",
      "    Evaluation Average Reward: -24006.65\n",
      "\n",
      "Running combination 372/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.999, 'alpha': 1.0, 'gamma': 0.9, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "372it [1:36:04, 17.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25550.71\n",
      "    Min Reward over last 50 episodes: -25787.79\n",
      "    Max Reward over last 50 episodes: -25313.63\n",
      "    Evaluation Average Reward: -26162.89\n",
      "\n",
      "Running combination 373/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 0.6, 'no_change_after_lap': 150}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "373it [1:36:21, 17.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17129.43\n",
      "    Min Reward over last 50 episodes: -17302.22\n",
      "    Max Reward over last 50 episodes: -16956.64\n",
      "    Evaluation Average Reward: -17014.86\n",
      "\n",
      "Running combination 374/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.0, 'gamma': 0.9, 'no_change_after_lap': 115}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "374it [1:36:38, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -25882.08\n",
      "    Min Reward over last 50 episodes: -25912.45\n",
      "    Max Reward over last 50 episodes: -25851.71\n",
      "    Evaluation Average Reward: -25637.91\n",
      "\n",
      "Running combination 375/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.2, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "375it [1:36:54, 16.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18015.70\n",
      "    Min Reward over last 50 episodes: -18105.15\n",
      "    Max Reward over last 50 episodes: -17926.24\n",
      "    Evaluation Average Reward: -18418.79\n",
      "\n",
      "Running combination 376/9555 with parameters: {'epsilon': 0.025, 'epsilon_decay': 0.995, 'alpha': 1.0, 'gamma': 0.2, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "376it [1:37:10, 16.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20797.00\n",
      "    Min Reward over last 50 episodes: -20839.79\n",
      "    Max Reward over last 50 episodes: -20754.21\n",
      "    Evaluation Average Reward: -20581.05\n",
      "\n",
      "Running combination 377/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.8, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "377it [1:37:26, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -18350.39\n",
      "    Min Reward over last 50 episodes: -18427.88\n",
      "    Max Reward over last 50 episodes: -18272.90\n",
      "    Evaluation Average Reward: -17973.29\n",
      "\n",
      "Running combination 378/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.4, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "378it [1:37:43, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -22985.38\n",
      "    Min Reward over last 50 episodes: -23151.46\n",
      "    Max Reward over last 50 episodes: -22819.29\n",
      "    Evaluation Average Reward: -23278.02\n",
      "\n",
      "Running combination 379/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.0, 'no_change_after_lap': 125}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "379it [1:38:00, 16.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -23126.68\n",
      "    Min Reward over last 50 episodes: -23197.01\n",
      "    Max Reward over last 50 episodes: -23056.35\n",
      "    Evaluation Average Reward: -23140.94\n",
      "\n",
      "Running combination 380/9555 with parameters: {'epsilon': 0.05, 'epsilon_decay': 0.995, 'alpha': 0.8, 'gamma': 0.2, 'no_change_after_lap': 100}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "380it [1:38:17, 16.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -29397.59\n",
      "    Min Reward over last 50 episodes: -29406.18\n",
      "    Max Reward over last 50 episodes: -29389.00\n",
      "    Evaluation Average Reward: -29565.15\n",
      "\n",
      "Running combination 381/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.999, 'alpha': 0.9, 'gamma': 0.2, 'no_change_after_lap': 135}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "381it [1:38:33, 16.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -20239.42\n",
      "    Min Reward over last 50 episodes: -20281.69\n",
      "    Max Reward over last 50 episodes: -20197.16\n",
      "    Evaluation Average Reward: -20389.51\n",
      "\n",
      "Running combination 382/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.2, 'gamma': 1.0, 'no_change_after_lap': 155}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "382it [1:38:50, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -16203.59\n",
      "    Min Reward over last 50 episodes: -16210.53\n",
      "    Max Reward over last 50 episodes: -16196.64\n",
      "    Evaluation Average Reward: -16307.04\n",
      "\n",
      "Running combination 383/9555 with parameters: {'epsilon': 0.2, 'epsilon_decay': 0.995, 'alpha': 0.0, 'gamma': 0.8, 'no_change_after_lap': 145}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "383it [1:39:09, 17.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -17935.33\n",
      "    Min Reward over last 50 episodes: -17981.93\n",
      "    Max Reward over last 50 episodes: -17888.73\n",
      "    Evaluation Average Reward: -17880.34\n",
      "\n",
      "Running combination 384/9555 with parameters: {'epsilon': 0.0, 'epsilon_decay': 0.99, 'alpha': 0.4, 'gamma': 0.6, 'no_change_after_lap': 120}\n",
      "    Training Episode 0/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "384it [1:39:27, 17.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluation Episode 0/1\n",
      "    Average Reward over last 50 episodes: -24457.95\n",
      "    Min Reward over last 50 episodes: -24488.01\n",
      "    Max Reward over last 50 episodes: -24427.88\n",
      "    Evaluation Average Reward: -24159.22\n",
      "\n",
      "Running combination 385/9555 with parameters: {'epsilon': 0.1, 'epsilon_decay': 0.99, 'alpha': 0.6, 'gamma': 0.0, 'no_change_after_lap': 140}\n",
      "    Training Episode 0/2\n"
     ]
    }
   ],
   "source": [
    "# if not os.path.exists(directory):\n",
    "#     os.makedirs(directory)\n",
    "\n",
    "def parameter_combinations(param_grid):\n",
    "    \"\"\"\n",
    "    Generate random combinations of parameters from the given parameter grid.\n",
    "    \n",
    "    Args:\n",
    "    - param_grid (dict): Dictionary containing hyperparameters and their possible values.\n",
    "\n",
    "    Yields:\n",
    "    - dict: Random combination of hyperparameters.\n",
    "    \"\"\"\n",
    "    # Connect to the SQLite database and fetch existing combinations\n",
    "    conn = sqlite3.connect(gs_db_name)\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Check if the table exists. If not, create an empty DataFrame.\n",
    "    c.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}';\")\n",
    "    if c.fetchone():\n",
    "        existing_combinations_df = pd.read_sql_query(f\"SELECT epsilon, epsilon_decay, alpha, gamma, no_change_after_lap FROM {table_name}\", conn)\n",
    "    else:\n",
    "        existing_combinations_df = pd.DataFrame()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # Generate and check combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    print(len(keys))\n",
    "    all_combinations = list(itertools.product(*values))\n",
    "    random.shuffle(all_combinations)\n",
    "\n",
    "    for combination in all_combinations:\n",
    "        param_dict = dict(zip(keys, combination))\n",
    "\n",
    "        # Check if this combination is already in the database\n",
    "        conditions = np.logical_and.reduce([existing_combinations_df[k] == v for k, v in param_dict.items()])\n",
    "        if not existing_combinations_df[conditions].empty:\n",
    "            print('SKIPPING')\n",
    "            continue  # Skip this combination if it's already in the database\n",
    "\n",
    "        yield param_dict\n",
    "\n",
    "# 1. Plot Episode Rewards\n",
    "def plot_rewards(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.title('Episode Rewards Over Time')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.show()\n",
    "\n",
    "# 2. Plot Average Episode Rewards\n",
    "def plot_avg_rewards(rewards, window=5):\n",
    "    averages = [np.mean(rewards[max(0, i-window+1):i+1]) for i in range(len(rewards))]\n",
    "    plt.plot(averages)\n",
    "    plt.title(f'Average Episode Rewards Over Last {window} Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.show()\n",
    "\n",
    "# 3. Implement Grid Search\n",
    "def grid_search(param_grid, num_episodes, eval_episodes):\n",
    "    conn = sqlite3.connect(gs_db_name)\n",
    "    c = conn.cursor()\n",
    "\n",
    "    total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "    print(f\"Total combinations: {total_combinations}\")\n",
    "    count = 0\n",
    "\n",
    "    for params in tqdm(parameter_combinations(param_grid)):\n",
    "        count += 1\n",
    "        print(f\"\\nRunning combination {count}/{total_combinations} with parameters: {params}\")\n",
    "\n",
    "        agent = TDLambdaAgent(epsilon=params['epsilon'], epsilon_decay=params['epsilon_decay'], alpha=params['alpha'], gamma=params['gamma'], no_change_after_lap=params['no_change_after_lap'])\n",
    "        \n",
    "        # Training phase\n",
    "        training_rewards = []  # List to store rewards from each episode\n",
    "        for episode in range(num_episodes):\n",
    "            if episode % 100 == 0:  # Print update every 100 episodes\n",
    "                print(f\"    Training Episode {episode}/{num_episodes}\")\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0 \n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "                reward, next_state, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                agent.update(state, action, reward, next_state)\n",
    "                # state = deepcopy(next_state)\n",
    "                state = next_state\n",
    "            training_rewards.append(episode_reward)  # Add the total reward for this episode to the list\n",
    "\n",
    "\n",
    "        # Evaluation phase\n",
    "        eval_rewards = []\n",
    "        for episode in range(eval_episodes):\n",
    "            if episode % 5 == 0:  # Print update every 100 episodes\n",
    "                print(f\"    Evaluation Episode {episode}/{eval_episodes}\")\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "                reward, next_state, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                # state = deepcopy(next_state)\n",
    "                state = next_state\n",
    "            eval_rewards.append(episode_reward)\n",
    "\n",
    "        # Compute metrics based on training rewards\n",
    "        avg_last_50 = np.mean(training_rewards[-50:])\n",
    "        min_last_50 = np.min(training_rewards[-50:])\n",
    "        max_last_50 = np.max(training_rewards[-50:])\n",
    "        eval_avg = np.mean(eval_rewards)\n",
    "        \n",
    "\n",
    "        # Print evaluation metrics\n",
    "        print(f\"    Average Reward over last 50 episodes: {avg_last_50:.2f}\")\n",
    "        print(f\"    Min Reward over last 50 episodes: {min_last_50:.2f}\")\n",
    "        print(f\"    Max Reward over last 50 episodes: {max_last_50:.2f}\")\n",
    "        print(f\"    Evaluation Average Reward: {eval_avg:.2f}\")\n",
    "\n",
    "        # Save metrics to database\n",
    "        c.execute(f\"\"\"INSERT INTO {table_name} (epsilon, epsilon_decay, alpha, gamma, no_change_after_lap, \n",
    "                                               avg_last_50, min_last_50, max_last_50, \n",
    "                                               eval_avg, overall_avg) \n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\", \n",
    "                  (params['epsilon'], params['epsilon_decay'], params['alpha'], params['gamma'], \n",
    "                   params['no_change_after_lap'], avg_last_50, min_last_50, max_last_50, \n",
    "                   eval_avg, np.mean(training_rewards)))\n",
    "\n",
    "        conn.commit()\n",
    "        \n",
    "    conn.close()\n",
    "\n",
    "grid_search(param_grid, num_episodes, eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be65329",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(gs_db_name)\n",
    "df = pd.read_sql_query(f\"SELECT COUNT(*) from {table_name}\", conn)\n",
    "print(df.shape)\n",
    "print(df)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb325730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nbformat --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dea73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_plot_top_combinations(db_name=gs_db_name, top_n=5):\n",
    "    # Connect to the database and fetch data\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    print(df.columns)\n",
    "    # Columns you want to include in the radar plot (i.e., your hyperparameters)\n",
    "    hyperparameter_columns = ['epsilon', 'epsilon_decay', 'alpha', 'gamma', 'no_change_after_lap', 'eval_avg']\n",
    "\n",
    "    # Normalize the hyperparameters based on all rows\n",
    "    normalized_df = (df[hyperparameter_columns] - df[hyperparameter_columns].min()) / (df[hyperparameter_columns].max() - df[hyperparameter_columns].min())\n",
    "    \n",
    "    # Sort by eval_reward (if it exists) and take top N\n",
    "    sort_column = 'eval_avg' if 'eval_avg' in df.columns else hyperparameter_columns[0]\n",
    "    top_rows_df = df.nlargest(top_n, sort_column)\n",
    "    top_normalized_df = normalized_df.loc[top_rows_df.index]\n",
    "\n",
    "\n",
    "    # Radar plot\n",
    "    from math import pi\n",
    "    labels = hyperparameter_columns\n",
    "    num_vars = len(labels)\n",
    "\n",
    "    angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "    angles += angles[:1]  # Adding the first angle at the end to close the circle\n",
    "\n",
    "    plt.figure(figsize=(10, 10), dpi=80)\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_xticks(angles[:-1])  # Removing the last angle for the labels\n",
    "    ax.set_xticklabels(labels, fontsize=12)\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    for i in range(top_n):\n",
    "        norm_values = top_normalized_df.iloc[i].values.flatten().tolist()\n",
    "        norm_values += norm_values[:1]  # Add the first value to the end to close the circle\n",
    "\n",
    "        true_values_list = top_rows_df.iloc[i][hyperparameter_columns].values.flatten().tolist()\n",
    "\n",
    "        param_string = ', '.join([f\"{col}={val:.2f}\" for col, val in zip(labels, true_values_list)])\n",
    "        line = ax.plot(angles, norm_values, linewidth=2, linestyle='solid', label=param_string)\n",
    "        ax.fill(angles, norm_values, alpha=0.1)\n",
    "\n",
    "        # Annotate with the true values\n",
    "        for angle, value, true_value in zip(angles, norm_values, true_values_list):\n",
    "            ax.annotate(f\"{true_value:.2f}\", \n",
    "                        xy=(angle, value), \n",
    "                        color=line[0].get_color(),\n",
    "                        ha='center', va='bottom')\n",
    "\n",
    "    plt.title(f'Top {top_n} Parameter Combinations', size=20, color='blue', y=1.1)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.show()\n",
    "\n",
    "    print(top_rows_df)\n",
    "\n",
    "radar_plot_top_combinations(top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb6419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
