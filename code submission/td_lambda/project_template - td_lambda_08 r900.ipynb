{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swifter\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ce7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import pickle\n",
    "import random\n",
    "import sqlite3\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def actRules(self, state):\n",
    "        return 1\n",
    "    \n",
    "    def actNaively(self):\n",
    "        return 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KEY PARAMETERS\n",
    "\n",
    "lambda_value = 0.8\n",
    "table_name = f\"gs_results_td_lambda_{lambda_value}\".replace(\".\", \"\")\n",
    "directory = \"e-greedy/agents\" #dir to create\n",
    "gs_db_name = 'e-greedy/grid_search_greedy_r900.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a9f54",
   "metadata": {},
   "source": [
    "### RADIUS SET TO 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341de44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Car:\n",
    "    def __init__(self, tyre=\"Intermediate\"):\n",
    "        self.default_tyre = tyre\n",
    "        self.possible_tyres = [\"Ultrasoft\", \"Soft\", \"Intermediate\", \"Fullwet\"]\n",
    "        self.pitstop_time = 23\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.change_tyre(self.default_tyre)\n",
    "    \n",
    "    \n",
    "    def degrade(self, w, r):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            self.condition *= (1 - 0.0050*w - (2500-r)/90000)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            self.condition *= (1 - 0.0051*w - (2500-r)/93000)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            self.condition *= (1 - 0.0052*abs(0.5-w) - (2500-r)/95000)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            self.condition *= (1 - 0.0053*(1-w) - (2500-r)/97000)\n",
    "        \n",
    "        \n",
    "    def change_tyre(self, new_tyre):\n",
    "        assert new_tyre in self.possible_tyres\n",
    "        self.tyre = new_tyre\n",
    "        self.condition = 1.00\n",
    "    \n",
    "    \n",
    "    def get_velocity(self):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            vel = 80.7*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            vel = 80.1*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            vel = 79.5*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            vel = 79.0*(0.2 + 0.8*self.condition**1.5)\n",
    "        return vel\n",
    "\n",
    "    \n",
    "class Track:\n",
    "    def __init__(self, car=Car()):\n",
    "        # self.radius and self.cur_weather are defined in self.reset()\n",
    "        self.total_laps = 162\n",
    "        self.car = car\n",
    "        self.possible_weather = [\"Dry\", \"20% Wet\", \"40% Wet\", \"60% Wet\", \"80% Wet\", \"100% Wet\"]\n",
    "        self.wetness = {\n",
    "            \"Dry\": 0.00, \"20% Wet\": 0.20, \"40% Wet\": 0.40, \"60% Wet\": 0.60, \"80% Wet\": 0.80, \"100% Wet\": 1.00\n",
    "        }\n",
    "        self.p_transition = {\n",
    "            \"Dry\": {\n",
    "                \"Dry\": 0.987, \"20% Wet\": 0.013, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"20% Wet\": {\n",
    "                \"Dry\": 0.012, \"20% Wet\": 0.975, \"40% Wet\": 0.013, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"40% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.012, \"40% Wet\": 0.975, \"60% Wet\": 0.013, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"60% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.012, \"60% Wet\": 0.975, \"80% Wet\": 0.013, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"80% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.012, \"80% Wet\": 0.975, \"100% Wet\": 0.013\n",
    "            },\n",
    "            \"100% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.012, \"100% Wet\": 0.988\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        # self.radius = np.random.randint(600,1201)\n",
    "        self.radius = 900\n",
    "        self.cur_weather = np.random.choice(self.possible_weather)\n",
    "        self.is_done = False\n",
    "        self.pitstop = False\n",
    "        self.laps_cleared = 0\n",
    "        self.car.reset()\n",
    "        return self._get_state()\n",
    "    \n",
    "    \n",
    "    def _get_state(self):\n",
    "        return [self.car.tyre, self.car.condition, self.cur_weather, self.radius, self.laps_cleared]\n",
    "        \n",
    "    \n",
    "    def transition(self, action=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (int):\n",
    "                0. Make a pitstop and fit new ‘Ultrasoft’ tyres\n",
    "                1. Make a pitstop and fit new ‘Soft’ tyres\n",
    "                2. Make a pitstop and fit new ‘Intermediate’ tyres\n",
    "                3. Make a pitstop and fit new ‘Fullwet’ tyres\n",
    "                4. Continue the next lap without changing tyres\n",
    "        \"\"\"\n",
    "        ## Pitstop time will be added on the first eight of the subsequent lap\n",
    "        time_taken = 0\n",
    "        if self.laps_cleared == int(self.laps_cleared):\n",
    "            if self.pitstop:\n",
    "                self.car.change_tyre(self.committed_tyre)\n",
    "                time_taken += self.car.pitstop_time\n",
    "                self.pitstop = False\n",
    "        \n",
    "        ## The environment is coded such that only an action taken at the start of the three-quarters mark of each lap matters\n",
    "        if self.laps_cleared - int(self.laps_cleared) == 0.75:\n",
    "            if action < 4:\n",
    "                self.pitstop = True\n",
    "                self.committed_tyre = self.car.possible_tyres[action]\n",
    "            else:\n",
    "                self.pitstop = False\n",
    "        \n",
    "        self.cur_weather = np.random.choice(\n",
    "            self.possible_weather, p=list(self.p_transition[self.cur_weather].values())\n",
    "        )\n",
    "        # we assume that degration happens only after a car has travelled the one-eighth lap\n",
    "        velocity = self.car.get_velocity()\n",
    "        time_taken += (2*np.pi*self.radius/8) / velocity\n",
    "        reward = 0 - time_taken\n",
    "        self.car.degrade(\n",
    "            w=self.wetness[self.cur_weather], r=self.radius\n",
    "        )\n",
    "        self.laps_cleared += 0.125\n",
    "        \n",
    "        if self.laps_cleared == self.total_laps:\n",
    "            self.is_done = True\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        return reward, next_state, self.is_done, velocity\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.transition(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bc5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_car = Car()\n",
    "env = Track(new_car)\n",
    "\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6d6a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sanity check..\n",
    "\n",
    "state = env.reset()    \n",
    "done = False\n",
    "G = 0\n",
    "while not done:\n",
    "    action = agent.actNaively()\n",
    "    reward, next_state, done, velocity = env.transition(action)\n",
    "    # added velocity for sanity check\n",
    "    # state = deepcopy(next_state)\n",
    "    state = next_state\n",
    "    G += reward\n",
    "\n",
    "print(\"G: %d\" % G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81524f23",
   "metadata": {},
   "source": [
    "## TDLambda Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebdc93",
   "metadata": {},
   "source": [
    "Discretizing States:\n",
    "\n",
    "Both the current state and the next_state are discretized using the _discretize_state method. This helps in managing the Q-values in a table format for states that are continuous or too granular.\n",
    "Initialization of Q-values and Eligibility Traces:\n",
    "\n",
    "If the current state or next_state is not in the Q-table (Q), it's added with an initial Q-value of zero for all actions.\n",
    "Similarly, if the current state is not in the eligibility traces table (E), it's added with an initial eligibility trace of zero for all actions.\n",
    "Temporal Difference (TD) Error Calculation:\n",
    "\n",
    "The agent calculates the best action for the next_state based on the current Q-values.\n",
    "The TD error is then computed. It's the difference between the expected Q-value (based on the received reward and the estimated future reward for next_state) and the current Q-value for the taken action at state.\n",
    "Eligibility Trace Update:\n",
    "\n",
    "The eligibility trace for the current state-action pair (state, action) is incremented by 1.\n",
    "This helps in assigning \"credit\" or \"blame\" for this state-action pair for any future reward or punishment. SARSA(λ) updates not just the current state-action pair but also the recent ones based on their eligibility traces.\n",
    "Ensure All States in Q have an Eligibility Trace:\n",
    "\n",
    "The agent makes sure that for all states in the Q-table, there's a corresponding entry in the eligibility traces table. If not, it adds them with a value of zero.\n",
    "Update Q-values Using TD Error and Eligibility Traces:\n",
    "\n",
    "The Q-values for all state-action pairs are updated based on the TD error and their respective eligibility traces. The greater the eligibility trace for a state-action pair, the more it gets updated.\n",
    "This is the heart of the SARSA(λ) algorithm, allowing the Q-value updates to be spread out not just to the current state-action pair but also to the previous ones based on their eligibility.\n",
    "Decay All Eligibility Traces:\n",
    "\n",
    "After updating the Q-values, the eligibility traces for all state-action pairs are decayed by a factor of gamma * td_lambda. This ensures that the traces for older state-action pairs decrease over time, giving more importance to recent state-action pairs for future Q-value updates.\n",
    "In essence, the update method blends the strengths of both SARSA and eligibility traces, providing a more sophisticated update mechanism that takes into account both immediate and more distant state-action pairs when adjusting Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDLambdaAgent:\n",
    "    def __init__(self, epsilon=0.1, epsilon_decay=0.995, alpha=0.1,\n",
    "                  gamma=0.99, td_lambda=lambda_value, n_actions=5,\n",
    "                    no_change_after_lap=150, state_space_discretization=100):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.n_actions))\n",
    "        self.E = defaultdict(lambda: np.zeros(self.n_actions))  # Eligibility traces\n",
    "        self.n_actions = n_actions\n",
    "        self.state_space_discretization = state_space_discretization\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.no_change_after_lap = no_change_after_lap\n",
    "        self.td_lambda = td_lambda\n",
    "        \n",
    "    def _discretize_state(self, state):\n",
    "        tyre, condition, weather, radius, laps_cleared = state\n",
    "        condition = int(condition * self.state_space_discretization)\n",
    "        \n",
    "        # Discretizing the radius as well by rounding to nearest hundred\n",
    "        radius = round(radius, -2)\n",
    "        return (tyre, condition, weather, radius, laps_cleared)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = self._discretize_state(state)\n",
    "        _, _, _, _, laps_cleared = state\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            if laps_cleared >= self.no_change_after_lap:\n",
    "                return 4  # Don't change tires\n",
    "            return np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.Q.get(state, np.zeros(self.n_actions)))\n",
    "            if laps_cleared >= self.no_change_after_lap and action < 4:\n",
    "                return 4  # Don't change tires\n",
    "            return action\n",
    "        \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        state = self._discretize_state(state)\n",
    "        next_state = self._discretize_state(next_state)\n",
    "        \n",
    "        best_next_action = np.argmax(self.Q[next_state])\n",
    "        td_error = reward + self.gamma * self.Q[next_state][best_next_action] - self.Q[state][action]\n",
    "        \n",
    "        # Increment the eligibility trace for the current state-action pair\n",
    "        self.E[state][action] += 1  \n",
    "        \n",
    "        # Update Q-values for all state-action pairs using TD error and the eligibility traces\n",
    "        for s, actions in self.Q.items():\n",
    "            for a in range(self.n_actions):\n",
    "                self.Q[s][a] += self.alpha * td_error * self.E[s][a]\n",
    "                \n",
    "                # Decay the eligibility trace for the state-action pair\n",
    "                self.E[s][a] *= self.gamma * self.td_lambda\n",
    "\n",
    "# Training the agent\n",
    "agent = TDLambdaAgent()\n",
    "\n",
    "num_episodes = 5\n",
    "eval_episodes = 1\n",
    "\n",
    "episode_rewards = []\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        reward, next_state, done, _ = env.step(action)\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        # state = deepcopy(next_state)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    episode_rewards.append(episode_reward)\n",
    "    agent.epsilon *= agent.epsilon_decay\n",
    "    \n",
    "# Evaluating the agent\n",
    "total_rewards = []\n",
    "for _ in tqdm(range(eval_episodes)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        reward, next_state, done, _ = env.step(action)\n",
    "        # state = deepcopy(next_state)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    total_rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Average reward over {eval_episodes} episodes: {np.mean(total_rewards)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5\n",
    "eval_episodes = 1\n",
    "\n",
    "# Grid Search\n",
    "\"\"\"\n",
    "param_grid: dict\n",
    "    A dictionary containing hyperparameters and their possible values to be explored during grid search. Each key represents a hyperparameter, and its corresponding value is a list of values that will be tested for that hyperparameter.\n",
    "\n",
    "    Parameters:\n",
    "    - epsilon: list of floats\n",
    "        The initial exploration rate for the epsilon-greedy policy. Values represent the probability at which the agent will explore, i.e., take random actions. Higher values mean more exploration and less exploitation.\n",
    "\n",
    "    - epsilon_decay: list of floats\n",
    "        The rate at which epsilon will be reduced after each episode. Values are in the range (0, 1) with values closer to 1 meaning a slower decay of epsilon over time.\n",
    "\n",
    "    - alpha: list of floats\n",
    "        The learning rate for the Q-learning algorithm. Determines to what extent newly acquired information overrides old information. A value of 0 would make the agent not learn anything, while a value of 1 would make the agent consider only the most recent information.\n",
    "\n",
    "    - gamma: list of floats\n",
    "        The discount factor for the Q-learning algorithm. Represents the agent's consideration for future rewards. A value of 0 makes the agent short-sighted by only considering current rewards, while a value close to 1 will make it aim for a long-term high reward.\n",
    "\n",
    "    - no_change_after_lap: list of integers\n",
    "        Specifies the lap number after which the agent will not change tires. If the current lap exceeds this number, actions to change tires will not be taken.\n",
    "\"\"\"\n",
    "param_grid = {\n",
    "    'epsilon': [0.0, 0.025, 0.05, 0.1, 0.2],\n",
    "    'epsilon_decay': [0.990, 0.995, 0.999],\n",
    "    'alpha': [0.0, 0.2, 0.4, 0.6, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0.0, 0.2, 0.4, 0.6, 0.8, 0.9, 1.0],\n",
    "    'no_change_after_lap': [x for x in range(100, 161, 5)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_database():\n",
    "    conn = sqlite3.connect(gs_db_name)\n",
    "    c = conn.cursor()\n",
    "    c.execute(f'''CREATE TABLE IF NOT EXISTS {table_name}\n",
    "                 (epsilon real, epsilon_decay real, alpha real, gamma real, no_change_after_lap integer, \n",
    "                  avg_last_50 real, min_last_50 real, max_last_50 real, \n",
    "                  eval_avg real, overall_avg real)''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "setup_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(directory):\n",
    "#     os.makedirs(directory)\n",
    "\n",
    "def parameter_combinations(param_grid):\n",
    "    \"\"\"\n",
    "    Generate random combinations of parameters from the given parameter grid.\n",
    "    \n",
    "    Args:\n",
    "    - param_grid (dict): Dictionary containing hyperparameters and their possible values.\n",
    "\n",
    "    Yields:\n",
    "    - dict: Random combination of hyperparameters.\n",
    "    \"\"\"\n",
    "    # Connect to the SQLite database and fetch existing combinations\n",
    "    conn = sqlite3.connect(gs_db_name)\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Check if the table exists. If not, create an empty DataFrame.\n",
    "    c.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}';\")\n",
    "    if c.fetchone():\n",
    "        existing_combinations_df = pd.read_sql_query(f\"SELECT epsilon, epsilon_decay, alpha, gamma, no_change_after_lap FROM {table_name}\", conn)\n",
    "    else:\n",
    "        existing_combinations_df = pd.DataFrame()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # Generate and check combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    print(len(keys))\n",
    "    all_combinations = list(itertools.product(*values))\n",
    "    random.shuffle(all_combinations)\n",
    "\n",
    "    for combination in all_combinations:\n",
    "        param_dict = dict(zip(keys, combination))\n",
    "\n",
    "        # Check if this combination is already in the database\n",
    "        conditions = np.logical_and.reduce([existing_combinations_df[k] == v for k, v in param_dict.items()])\n",
    "        if not existing_combinations_df[conditions].empty:\n",
    "            print('SKIPPING')\n",
    "            continue  # Skip this combination if it's already in the database\n",
    "\n",
    "        yield param_dict\n",
    "\n",
    "# 1. Plot Episode Rewards\n",
    "def plot_rewards(rewards):\n",
    "    plt.plot(rewards)\n",
    "    plt.title('Episode Rewards Over Time')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.show()\n",
    "\n",
    "# 2. Plot Average Episode Rewards\n",
    "def plot_avg_rewards(rewards, window=5):\n",
    "    averages = [np.mean(rewards[max(0, i-window+1):i+1]) for i in range(len(rewards))]\n",
    "    plt.plot(averages)\n",
    "    plt.title(f'Average Episode Rewards Over Last {window} Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.show()\n",
    "\n",
    "# 3. Implement Grid Search\n",
    "def grid_search(param_grid, num_episodes, eval_episodes):\n",
    "    conn = sqlite3.connect(gs_db_name)\n",
    "    c = conn.cursor()\n",
    "\n",
    "    total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "    print(f\"Total combinations: {total_combinations}\")\n",
    "    count = 0\n",
    "\n",
    "    for params in tqdm(parameter_combinations(param_grid)):\n",
    "        count += 1\n",
    "        print(f\"\\nRunning combination {count}/{total_combinations} with parameters: {params}\")\n",
    "\n",
    "        agent = TDLambdaAgent(epsilon=params['epsilon'], epsilon_decay=params['epsilon_decay'], alpha=params['alpha'], gamma=params['gamma'], no_change_after_lap=params['no_change_after_lap'])\n",
    "        \n",
    "        # Training phase\n",
    "        training_rewards = []  # List to store rewards from each episode\n",
    "        for episode in range(num_episodes):\n",
    "            if episode % 100 == 0:  # Print update every 100 episodes\n",
    "                print(f\"    Training Episode {episode}/{num_episodes}\")\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0 \n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "                reward, next_state, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                agent.update(state, action, reward, next_state)\n",
    "                # state = deepcopy(next_state)\n",
    "                state = next_state\n",
    "            training_rewards.append(episode_reward)  # Add the total reward for this episode to the list\n",
    "\n",
    "\n",
    "        # Evaluation phase\n",
    "        eval_rewards = []\n",
    "        for episode in range(eval_episodes):\n",
    "            if episode % 5 == 0:  # Print update every 100 episodes\n",
    "                print(f\"    Evaluation Episode {episode}/{eval_episodes}\")\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "                reward, next_state, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                # state = deepcopy(next_state)\n",
    "                state = next_state\n",
    "            eval_rewards.append(episode_reward)\n",
    "\n",
    "        # Compute metrics based on training rewards\n",
    "        avg_last_50 = np.mean(training_rewards[-50:])\n",
    "        min_last_50 = np.min(training_rewards[-50:])\n",
    "        max_last_50 = np.max(training_rewards[-50:])\n",
    "        eval_avg = np.mean(eval_rewards)\n",
    "        \n",
    "\n",
    "        # Print evaluation metrics\n",
    "        print(f\"    Average Reward over last 50 episodes: {avg_last_50:.2f}\")\n",
    "        print(f\"    Min Reward over last 50 episodes: {min_last_50:.2f}\")\n",
    "        print(f\"    Max Reward over last 50 episodes: {max_last_50:.2f}\")\n",
    "        print(f\"    Evaluation Average Reward: {eval_avg:.2f}\")\n",
    "\n",
    "        # Save metrics to database\n",
    "        c.execute(f\"\"\"INSERT INTO {table_name} (epsilon, epsilon_decay, alpha, gamma, no_change_after_lap, \n",
    "                                               avg_last_50, min_last_50, max_last_50, \n",
    "                                               eval_avg, overall_avg) \n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\", \n",
    "                  (params['epsilon'], params['epsilon_decay'], params['alpha'], params['gamma'], \n",
    "                   params['no_change_after_lap'], avg_last_50, min_last_50, max_last_50, \n",
    "                   eval_avg, np.mean(training_rewards)))\n",
    "\n",
    "        conn.commit()\n",
    "        \n",
    "    conn.close()\n",
    "\n",
    "grid_search(param_grid, num_episodes, eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be65329",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(gs_db_name)\n",
    "df = pd.read_sql_query(f\"SELECT COUNT(*) from {table_name}\", conn)\n",
    "print(df.shape)\n",
    "print(df)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb325730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nbformat --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dea73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radar_plot_top_combinations(db_name=gs_db_name, top_n=5):\n",
    "    # Connect to the database and fetch data\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    df = pd.read_sql_query(f\"SELECT * from {table_name}\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    print(df.columns)\n",
    "    # Columns you want to include in the radar plot (i.e., your hyperparameters)\n",
    "    hyperparameter_columns = ['epsilon', 'epsilon_decay', 'alpha', 'gamma', 'no_change_after_lap', 'eval_avg']\n",
    "\n",
    "    # Normalize the hyperparameters based on all rows\n",
    "    normalized_df = (df[hyperparameter_columns] - df[hyperparameter_columns].min()) / (df[hyperparameter_columns].max() - df[hyperparameter_columns].min())\n",
    "    \n",
    "    # Sort by eval_reward (if it exists) and take top N\n",
    "    sort_column = 'eval_avg' if 'eval_avg' in df.columns else hyperparameter_columns[0]\n",
    "    top_rows_df = df.nlargest(top_n, sort_column)\n",
    "    top_normalized_df = normalized_df.loc[top_rows_df.index]\n",
    "\n",
    "\n",
    "    # Radar plot\n",
    "    from math import pi\n",
    "    labels = hyperparameter_columns\n",
    "    num_vars = len(labels)\n",
    "\n",
    "    angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "    angles += angles[:1]  # Adding the first angle at the end to close the circle\n",
    "\n",
    "    plt.figure(figsize=(10, 10), dpi=80)\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    ax.set_xticks(angles[:-1])  # Removing the last angle for the labels\n",
    "    ax.set_xticklabels(labels, fontsize=12)\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    for i in range(top_n):\n",
    "        norm_values = top_normalized_df.iloc[i].values.flatten().tolist()\n",
    "        norm_values += norm_values[:1]  # Add the first value to the end to close the circle\n",
    "\n",
    "        true_values_list = top_rows_df.iloc[i][hyperparameter_columns].values.flatten().tolist()\n",
    "\n",
    "        param_string = ', '.join([f\"{col}={val:.2f}\" for col, val in zip(labels, true_values_list)])\n",
    "        line = ax.plot(angles, norm_values, linewidth=2, linestyle='solid', label=param_string)\n",
    "        ax.fill(angles, norm_values, alpha=0.1)\n",
    "\n",
    "        # Annotate with the true values\n",
    "        for angle, value, true_value in zip(angles, norm_values, true_values_list):\n",
    "            ax.annotate(f\"{true_value:.2f}\", \n",
    "                        xy=(angle, value), \n",
    "                        color=line[0].get_color(),\n",
    "                        ha='center', va='bottom')\n",
    "\n",
    "    plt.title(f'Top {top_n} Parameter Combinations', size=20, color='blue', y=1.1)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.show()\n",
    "\n",
    "    print(top_rows_df)\n",
    "\n",
    "radar_plot_top_combinations(top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb6419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
